{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "%pip install \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Warning control\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hemant.Singhsidar\\Desktop\\Unstructured\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "import json\n",
    "\n",
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.partition.image import partition_image\n",
    "from unstructured.staging.base import dict_to_elements, elements_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"CaseStudies.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "poppler_path = r\"C:/Users/Hemant.Singhsidar/Downloads/Release-24.08.0-0/poppler-24.08.0/Library/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# elements = partition_pdf(filename=filename,\n",
    "#                          infer_table_structure=True,\n",
    "#                          strategy='hi_res',\n",
    "#            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"PDF Pages/2111.15664v5-Pages/page_1.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.image import partition_image\n",
    "\n",
    "elements = partition_image(filename=filename,\n",
    "                         infer_table_structure=True,\n",
    "                         strategy='hi_res',\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.Title at 0x1d9e9ac1c00>,\n",
       " <unstructured.documents.elements.Text at 0x1d9dae09d50>,\n",
       " <unstructured.documents.elements.Header at 0x1d9e9ac19f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d9e9ac17b0>,\n",
       " <unstructured.documents.elements.Table at 0x1d9e9ac1db0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d9e9ac2170>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d9e9ac2440>,\n",
       " <unstructured.documents.elements.Title at 0x1d9e9ac2620>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x1d9e9ac2800>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d9e9ac29e0>,\n",
       " <unstructured.documents.elements.Text at 0x1d9dadc4640>,\n",
       " <unstructured.documents.elements.ListItem at 0x1d9e9ac2bc0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"type\": \"Title\",\n",
      "    \"element_id\": \"194bb9484908dab35dc6eb221f16ea64\",\n",
      "    \"text\": \"OCR-free Document Understanding Transformer\",\n",
      "    \"metadata\": {\n",
      "      \"detection_class_prob\": 0.5052953958511353,\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            91.509521484375,\n",
      "            89.04460144042969\n",
      "          ],\n",
      "          [\n",
      "            91.509521484375,\n",
      "            130.8874969482422\n",
      "          ],\n",
      "          [\n",
      "            1053.1043701171875,\n",
      "            130.8874969482422\n",
      "          ],\n",
      "          [\n",
      "            1053.1043701171875,\n",
      "            89.04460144042969\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1150,\n",
      "        \"layout_height\": 1709\n",
      "      },\n",
      "      \"last_modified\": \"2025-02-10T13:44:24\",\n",
      "      \"filetype\": \"PNG\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"file_directory\": \"PDF Pages/2111.15664v5-Pages\",\n",
      "      \"filename\": \"page_1.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"UncategorizedText\",\n",
      "    \"element_id\": \"8ce57af859921abcf55a0e8ec475bfc4\",\n",
      "    \"text\": \"2 202\",\n",
      "    \"metadata\": {\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            52.0,\n",
      "            117.0\n",
      "          ],\n",
      "          [\n",
      "            52.0,\n",
      "            224.0\n",
      "          ],\n",
      "          [\n",
      "            89.0,\n",
      "            224.0\n",
      "          ],\n",
      "          [\n",
      "            89.0,\n",
      "            117.0\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1150,\n",
      "        \"layout_height\": 1709\n",
      "      },\n",
      "      \"last_modified\": \"2025-02-10T13:44:24\",\n",
      "      \"filetype\": \"PNG\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"parent_id\": \"194bb9484908dab35dc6eb221f16ea64\",\n",
      "      \"file_directory\": \"PDF Pages/2111.15664v5-Pages\",\n",
      "      \"filename\": \"page_1.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Header\",\n",
      "    \"element_id\": \"d9cb9794a60b86a88fbfc9cfb95251aa\",\n",
      "    \"text\": \"arXiv 2111.15664v5 [cs.LG] 6 Oct\",\n",
      "    \"metadata\": {\n",
      "      \"detection_class_prob\": 0.28698790073394775,\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            20.199811935424805,\n",
      "            191.49533081054688\n",
      "          ],\n",
      "          [\n",
      "            20.199811935424805,\n",
      "            1118.8106689453125\n",
      "          ],\n",
      "          [\n",
      "            118.02880859375,\n",
      "            1118.8106689453125\n",
      "          ],\n",
      "          [\n",
      "            118.02880859375,\n",
      "            191.49533081054688\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1150,\n",
      "        \"layout_height\": 1709\n",
      "      },\n",
      "      \"last_modified\": \"2025-02-10T13:44:24\",\n",
      "      \"filetype\": \"PNG\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"file_directory\": \"PDF Pages/2111.15664v5-Pages\",\n",
      "      \"filename\": \"page_1.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"NarrativeText\",\n",
      "    \"element_id\": \"7d7889fe6920195f08610b69fb64c4ac\",\n",
      "    \"text\": \"Geewook Kim!*, Teakgyu Hong*?, Moonbin Yim?, Jeongyeon Nam!, Jinyoung Park\\u00ae!, Jinyeong Yim\\u00ae!, Wonseok Hwang\\u2019, Sangdoo Yun\\u2019, Dongyoon Han*, and Seunghyun Park!\",\n",
      "    \"metadata\": {\n",
      "      \"detection_class_prob\": 0.838838517665863,\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            148.66900634765625,\n",
      "            197.10777282714844\n",
      "          ],\n",
      "          [\n",
      "            148.66900634765625,\n",
      "            292.4017028808594\n",
      "          ],\n",
      "          [\n",
      "            1005.0565185546875,\n",
      "            292.4017028808594\n",
      "          ],\n",
      "          [\n",
      "            1005.0565185546875,\n",
      "            197.10777282714844\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1150,\n",
      "        \"layout_height\": 1709\n",
      "      },\n",
      "      \"last_modified\": \"2025-02-10T13:44:24\",\n",
      "      \"filetype\": \"PNG\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"parent_id\": \"d9cb9794a60b86a88fbfc9cfb95251aa\",\n",
      "      \"file_directory\": \"PDF Pages/2111.15664v5-Pages\",\n",
      "      \"filename\": \"page_1.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Table\",\n",
      "    \"element_id\": \"34d555696d48156eac62d2a4600ae701\",\n",
      "    \"text\": \"'NAVER CLOVA ?NAVER Search 'NAVER. AI Lab \\u2018Upstage 5Tmax \\u00aeGoogle \\\"LBox\",\n",
      "    \"metadata\": {\n",
      "      \"detection_class_prob\": 0.4464171528816223,\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            224.56277465820312,\n",
      "            319.9187927246094\n",
      "          ],\n",
      "          [\n",
      "            224.56277465820312,\n",
      "            382.0470886230469\n",
      "          ],\n",
      "          [\n",
      "            925.2555541992188,\n",
      "            382.0470886230469\n",
      "          ],\n",
      "          [\n",
      "            925.2555541992188,\n",
      "            319.9187927246094\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1150,\n",
      "        \"layout_height\": 1709\n",
      "      },\n",
      "      \"last_modified\": \"2025-02-10T13:44:24\",\n",
      "      \"text_as_html\": \"<table><tbody><tr><td></td><td>4Upstage</td><td>5Tmax</td><td>\\u00ae Google</td><td>TL Box</td><td></td></tr></tbody></table>\",\n",
      "      \"filetype\": \"PNG\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"parent_id\": \"d9cb9794a60b86a88fbfc9cfb95251aa\",\n",
      "      \"file_directory\": \"PDF Pages/2111.15664v5-Pages\",\n",
      "      \"filename\": \"page_1.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"NarrativeText\",\n",
      "    \"element_id\": \"9768bee2b0fa0542070c6e211a1f1b39\",\n",
      "    \"text\": \"Abstract. Understanding document images (e.g., invoices) is a core but challenging task since it requires complex functions such as reading tert and a holistic understanding of the document. Current Visual Document Understanding (VDU) methods outsource the task of reading text to off- the-shelf Optical Character Recognition (OCR) engines and focus on the understanding task with the OCR outputs. Although such OCR-based approaches have shown promising performance, they suffer from 1) high computational costs for using OCR; 2) inflexibility of OCR models on languages or types of documents; 3) OCR error propagation to the sul sequent process. To address these issues, in this paper, we introduce a novel OCR-free VDU model named Donut, which stands for Document understanding transformer. As the first step in OCR-free VDU research, we propose a simple architecture (i.e., Transformer) with a pre-training objective (i.e., cross-entropy loss). Donut is conceptually simple yet ef- fective. Through extensive experiments and analyses, we show a simple OCR-free VDU model, Donut, achieves state-of-the-art performances on various VDU tasks in terms of both speed and accuracy. In addition, we offer a synthetic data generator that helps the model pre-training to be flexible in various languages and domains. The code, trained model, and synthetic data are available at https: //github.com/clovaai/donut.\",\n",
      "    \"metadata\": {\n",
      "      \"detection_class_prob\": 0.9337868094444275,\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            171.88929748535156,\n",
      "            490.8583984375\n",
      "          ],\n",
      "          [\n",
      "            171.88929748535156,\n",
      "            1089.934326171875\n",
      "          ],\n",
      "          [\n",
      "            975.2742309570312,\n",
      "            1089.934326171875\n",
      "          ],\n",
      "          [\n",
      "            975.2742309570312,\n",
      "            490.8583984375\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1150,\n",
      "        \"layout_height\": 1709\n",
      "      },\n",
      "      \"last_modified\": \"2025-02-10T13:44:24\",\n",
      "      \"filetype\": \"PNG\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"parent_id\": \"d9cb9794a60b86a88fbfc9cfb95251aa\",\n",
      "      \"file_directory\": \"PDF Pages/2111.15664v5-Pages\",\n",
      "      \"filename\": \"page_1.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"NarrativeText\",\n",
      "    \"element_id\": \"c39ba1f08b3f96622327bf09cce9f918\",\n",
      "    \"text\": \"Keywords: Visual Document Understanding, Document Information Extraction, Optical Character Recognition, End-to-End Transformer\",\n",
      "    \"metadata\": {\n",
      "      \"detection_class_prob\": 0.8971932530403137,\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            170.68533325195312,\n",
      "            1125.527099609375\n",
      "          ],\n",
      "          [\n",
      "            170.68533325195312,\n",
      "            1183.0706787109375\n",
      "          ],\n",
      "          [\n",
      "            982.3406372070312,\n",
      "            1183.0706787109375\n",
      "          ],\n",
      "          [\n",
      "            982.3406372070312,\n",
      "            1125.527099609375\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1150,\n",
      "        \"layout_height\": 1709\n",
      "      },\n",
      "      \"last_modified\": \"2025-02-10T13:44:24\",\n",
      "      \"filetype\": \"PNG\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"parent_id\": \"d9cb9794a60b86a88fbfc9cfb95251aa\",\n",
      "      \"file_directory\": \"PDF Pages/2111.15664v5-Pages\",\n",
      "      \"filename\": \"page_1.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Title\",\n",
      "    \"element_id\": \"de4f9202045fbdc02a8c3cdb5dc3f338\",\n",
      "    \"text\": \"1 Introduction\",\n",
      "    \"metadata\": {\n",
      "      \"detection_class_prob\": 0.8171036839485168,\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            97.74453735351562,\n",
      "            1254.23779296875\n",
      "          ],\n",
      "          [\n",
      "            97.74453735351562,\n",
      "            1286.5723876953125\n",
      "          ],\n",
      "          [\n",
      "            356.880126953125,\n",
      "            1286.5723876953125\n",
      "          ],\n",
      "          [\n",
      "            356.880126953125,\n",
      "            1254.23779296875\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1150,\n",
      "        \"layout_height\": 1709\n",
      "      },\n",
      "      \"last_modified\": \"2025-02-10T13:44:24\",\n",
      "      \"filetype\": \"PNG\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"parent_id\": \"d9cb9794a60b86a88fbfc9cfb95251aa\",\n",
      "      \"file_directory\": \"PDF Pages/2111.15664v5-Pages\",\n",
      "      \"filename\": \"page_1.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"NarrativeText\",\n",
      "    \"element_id\": \"ed99d7d760774e6cfbbc61f584c808da\",\n",
      "    \"text\": \"Document images, such as commercial invoices, receipts, and business cards, are easy to find in modern working environments. To extract useful informa- tion from such document images, Visual Document Understanding (VDU) has not been only an essential task for industry, but also a challenging topic for re- searchers, with applications including document classification [27,1], information extraction [22,42], and visual question answering [44,57].\",\n",
      "    \"metadata\": {\n",
      "      \"detection_class_prob\": 0.9525009393692017,\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            95.58577728271484,\n",
      "            1334.196044921875\n",
      "          ],\n",
      "          [\n",
      "            95.58577728271484,\n",
      "            1527.7562255859375\n",
      "          ],\n",
      "          [\n",
      "            1062.2027587890625,\n",
      "            1527.7562255859375\n",
      "          ],\n",
      "          [\n",
      "            1062.2027587890625,\n",
      "            1334.196044921875\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1150,\n",
      "        \"layout_height\": 1709\n",
      "      },\n",
      "      \"last_modified\": \"2025-02-10T13:44:24\",\n",
      "      \"filetype\": \"PNG\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"parent_id\": \"de4f9202045fbdc02a8c3cdb5dc3f338\",\n",
      "      \"file_directory\": \"PDF Pages/2111.15664v5-Pages\",\n",
      "      \"filename\": \"page_1.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"ListItem\",\n",
      "    \"element_id\": \"db65426c7e6d554a0d274422e457d30c\",\n",
      "    \"text\": \"* Corresponding author: gwkim.rsrch@gmail.com\",\n",
      "    \"metadata\": {\n",
      "      \"detection_class_prob\": 0.7557909488677979,\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            130.57669067382812,\n",
      "            1563.127685546875\n",
      "          ],\n",
      "          [\n",
      "            130.57669067382812,\n",
      "            1590.1905517578125\n",
      "          ],\n",
      "          [\n",
      "            690.0404663085938,\n",
      "            1590.1905517578125\n",
      "          ],\n",
      "          [\n",
      "            690.0404663085938,\n",
      "            1563.127685546875\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1150,\n",
      "        \"layout_height\": 1709\n",
      "      },\n",
      "      \"last_modified\": \"2025-02-10T13:44:24\",\n",
      "      \"filetype\": \"PNG\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"parent_id\": \"de4f9202045fbdc02a8c3cdb5dc3f338\",\n",
      "      \"file_directory\": \"PDF Pages/2111.15664v5-Pages\",\n",
      "      \"filename\": \"page_1.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"UncategorizedText\",\n",
      "    \"element_id\": \"66a392268ecac085f65dde0c640ee59c\",\n",
      "    \"text\": \"}\",\n",
      "    \"metadata\": {\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            135.0,\n",
      "            1592.0\n",
      "          ],\n",
      "          [\n",
      "            135.0,\n",
      "            1607.0\n",
      "          ],\n",
      "          [\n",
      "            141.0,\n",
      "            1607.0\n",
      "          ],\n",
      "          [\n",
      "            141.0,\n",
      "            1592.0\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1150,\n",
      "        \"layout_height\": 1709\n",
      "      },\n",
      "      \"last_modified\": \"2025-02-10T13:44:24\",\n",
      "      \"filetype\": \"PNG\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"parent_id\": \"de4f9202045fbdc02a8c3cdb5dc3f338\",\n",
      "      \"file_directory\": \"PDF Pages/2111.15664v5-Pages\",\n",
      "      \"filename\": \"page_1.png\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"ListItem\",\n",
      "    \"element_id\": \"b9919cfad25bc2b34d28edeb490b2470\",\n",
      "    \"text\": \"This work was done while the authors were at NAVER CLOVA.\",\n",
      "    \"metadata\": {\n",
      "      \"detection_class_prob\": 0.803264319896698,\n",
      "      \"coordinates\": {\n",
      "        \"points\": [\n",
      "          [\n",
      "            142.7620849609375,\n",
      "            1593.042236328125\n",
      "          ],\n",
      "          [\n",
      "            142.7620849609375,\n",
      "            1620.5103759765625\n",
      "          ],\n",
      "          [\n",
      "            871.6179809570312,\n",
      "            1620.5103759765625\n",
      "          ],\n",
      "          [\n",
      "            871.6179809570312,\n",
      "            1593.042236328125\n",
      "          ]\n",
      "        ],\n",
      "        \"system\": \"PixelSpace\",\n",
      "        \"layout_width\": 1150,\n",
      "        \"layout_height\": 1709\n",
      "      },\n",
      "      \"last_modified\": \"2025-02-10T13:44:24\",\n",
      "      \"filetype\": \"PNG\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"parent_id\": \"de4f9202045fbdc02a8c3cdb5dc3f338\",\n",
      "      \"file_directory\": \"PDF Pages/2111.15664v5-Pages\",\n",
      "      \"filename\": \"page_1.png\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "{'Title', 'NarrativeText', 'ListItem', 'UncategorizedText', 'Header', 'Table'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "element_dict = [el.to_dict() for el in elements]\n",
    "output = json.dumps(element_dict, indent=2)\n",
    "print(output)\n",
    "\n",
    "unique_types = set()\n",
    "\n",
    "for item in element_dict:\n",
    "    unique_types.add(item['type'])\n",
    "\n",
    "print(unique_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total bounding boxes extracted: 1\n",
      "{\n",
      "  \"34d555696d48156eac62d2a4600ae701\": {\n",
      "    \"type\": \"Table\",\n",
      "    \"bbox\": [\n",
      "      224.56277465820312,\n",
      "      319.9187927246094,\n",
      "      925.2555541992188,\n",
      "      382.0470886230469\n",
      "    ],\n",
      "    \"page_number\": 1,\n",
      "    \"filename\": \"page_1.png\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Convert elements to dictionary format\n",
    "elements_dict = [el.to_dict() for el in elements]  # Assuming `elements` is a list of objects\n",
    "\n",
    "# Target types to extract\n",
    "target_types = {\"Table\", \"Image\"}\n",
    "\n",
    "# Dictionary to store bounding boxes\n",
    "bounding_boxes = {}\n",
    "\n",
    "# Counter for total extracted boxes\n",
    "total_boxes = 0\n",
    "\n",
    "for item in elements_dict:\n",
    "    if item[\"type\"] in target_types:\n",
    "        # Extract bounding box coordinates\n",
    "        points = item[\"metadata\"][\"coordinates\"][\"points\"]\n",
    "        \n",
    "        # Calculate bounding box as [x_min, y_min, x_max, y_max]\n",
    "        x_min = min(p[0] for p in points)\n",
    "        y_min = min(p[1] for p in points)\n",
    "        x_max = max(p[0] for p in points)\n",
    "        y_max = max(p[1] for p in points)\n",
    "        \n",
    "        # Store bounding box using element_id for reference\n",
    "        bounding_boxes[item[\"element_id\"]] = {\n",
    "            \"type\": item[\"type\"],\n",
    "            \"bbox\": [x_min, y_min, x_max, y_max],\n",
    "            \"page_number\": item[\"metadata\"][\"page_number\"],\n",
    "            \"filename\": item[\"metadata\"][\"filename\"]\n",
    "        }\n",
    "        \n",
    "        total_boxes += 1\n",
    "\n",
    "# Print total count of bounding boxes\n",
    "print(f\"Total bounding boxes extracted: {total_boxes}\")\n",
    "\n",
    "# Print bounding boxes data\n",
    "print(json.dumps(bounding_boxes, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'34d555696d48156eac62d2a4600ae701': {'type': 'Table',\n",
       "  'bbox': [np.float64(224.56277465820312),\n",
       "   np.float64(319.9187927246094),\n",
       "   np.float64(925.2555541992188),\n",
       "   np.float64(382.0470886230469)],\n",
       "  'page_number': 1,\n",
       "  'filename': 'page_1.png'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "\n",
    "def extract_and_save_tables(pdf_path, bounding_boxes):\n",
    "    \"\"\"\n",
    "    Extract tables from a PDF using bounding boxes and save them as images.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the input PDF file.\n",
    "        bounding_boxes (dict): Dictionary containing bounding box information.\n",
    "    \"\"\"\n",
    "    # Convert PDF to images (one per page)\n",
    "    images = convert_from_path(pdf_path, dpi=300)\n",
    "\n",
    "    for table_id, data in bounding_boxes.items():\n",
    "        page_num = data[\"page_number\"] - 1  # Adjust for zero-based index\n",
    "        filename = os.path.splitext(os.path.basename(data[\"filename\"]))[0]\n",
    "        bbox = data[\"bbox\"]\n",
    "        element_type = data[\"type\"]\n",
    "\n",
    "        # Define save directory\n",
    "        save_dir = f\"{filename}-{element_type}\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Open the corresponding page image\n",
    "        page_img = images[page_num]\n",
    "\n",
    "        # Crop the table region\n",
    "        cropped_img = page_img.crop((bbox[0], bbox[1], bbox[2], bbox[3]))\n",
    "\n",
    "        # Save the cropped image\n",
    "        save_path = os.path.join(save_dir, f\"{table_id}_{data['page_number']}.png\")\n",
    "        cropped_img.save(save_path, \"PNG\")\n",
    "\n",
    "        print(f\"Saved: {save_path}\")\n",
    "\n",
    "# Example Usage\n",
    "pdf_path = \"CaseStudies.pdf\"  # Path to your PDF file\n",
    "\n",
    "extract_and_save_tables(pdf_path, bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def annotate_tables(pdf_path, output_data):\n",
    "    \"\"\"\n",
    "    Extracts and annotates tables in a PDF document using bounding boxes from extracted data.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the input PDF file.\n",
    "        output_data (list): List of dictionaries containing element details.\n",
    "    \"\"\"\n",
    "    # Convert PDF to images\n",
    "    images = convert_from_path(pdf_path, dpi=300)\n",
    "\n",
    "    # Process each detected element\n",
    "    for element in output_data:\n",
    "        if element[\"type\"] == \"Table\":  # Only process tables\n",
    "            page_num = element[\"metadata\"][\"page_number\"] - 1  # Convert to zero-based index\n",
    "            bbox_points = element[\"metadata\"][\"coordinates\"][\"points\"]\n",
    "            filename = os.path.splitext(os.path.basename(element[\"metadata\"][\"filename\"]))[0]\n",
    "\n",
    "            # Define save directory\n",
    "            save_dir = f\"{filename}-AnnotatedTables\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            # Open corresponding page image\n",
    "            page_img = images[page_num].convert(\"RGB\")\n",
    "\n",
    "            # Draw bounding box annotation\n",
    "            draw = ImageDraw.Draw(page_img)\n",
    "            x_min, y_min = bbox_points[0]\n",
    "            x_max, y_max = bbox_points[2]\n",
    "            draw.rectangle([x_min, y_min, x_max, y_max], outline=\"red\", width=3)\n",
    "\n",
    "            # Save annotated image\n",
    "            save_path = os.path.join(save_dir, f\"{element['element_id']}_{page_num + 1}_annotated.png\")\n",
    "            page_img.save(save_path, \"PNG\")\n",
    "\n",
    "            print(f\"Saved annotated image: {save_path}\")\n",
    "\n",
    "# Example Usage\n",
    "pdf_path = \"CaseStudies.pdf\"  # Path to the PDF file\n",
    "output_data = json.loads(output)\n",
    "# annotate_tables(pdf_path, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in output_data:\n",
    "        if element[\"type\"] == \"Table\":  # Only process tables\n",
    "            page_num = element[\"metadata\"][\"page_number\"] - 1  # Convert to zero-based index\n",
    "            bbox_points = element[\"metadata\"][\"coordinates\"][\"points\"]\n",
    "            filename = os.path.splitext(os.path.basename(element[\"metadata\"][\"filename\"]))[0]\n",
    "            x_min, y_min = bbox_points[0]\n",
    "            x_max, y_max = bbox_points[2]\n",
    "            print(bbox_points)\n",
    "            print(page_num)\n",
    "            print(filename)\n",
    "            print(x_min, y_min, x_max, y_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "from pdf2image import convert_from_path\n",
    "import numpy as np\n",
    "\n",
    "# Load JSON if it's stored as a string\n",
    "output_json_path = \"output.json\"  # Replace with your actual file path\n",
    "\n",
    "# Check if the output is a JSON file, and load it properly\n",
    "if os.path.exists(output_json_path):\n",
    "    with open(output_json_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        output = json.load(file)  # Ensure it's a dictionary or list\n",
    "elif isinstance(output, str):  # If it's a string, parse it as JSON\n",
    "    output = json.loads(output)\n",
    "\n",
    "def annotate_tables(pdf_path, output_data):\n",
    "    \"\"\"\n",
    "    Annotates detected tables on PDF pages with bounding boxes instead of cropping.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the input PDF file.\n",
    "        output_data (list): List of dictionaries containing extracted element details.\n",
    "    \"\"\"\n",
    "    if not isinstance(output_data, list):  # Ensure it's a list\n",
    "        raise ValueError(\"Expected 'output_data' to be a list of dictionaries.\")\n",
    "\n",
    "    # Convert PDF to images\n",
    "    images = convert_from_path(pdf_path, dpi=300)\n",
    "    print(f\"✅ PDF converted to {len(images)} images.\")\n",
    "\n",
    "    if not images:\n",
    "        print(\"❌ Error: No images extracted from the PDF!\")\n",
    "        return\n",
    "\n",
    "    # Create a directory for annotated pages\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    save_dir = f\"{pdf_name}-AnnotatedPages\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Process each detected table and annotate\n",
    "    for element in output_data:\n",
    "        if isinstance(element, dict) and element.get(\"type\") == \"Table\":  # Ensure it's a valid table entry\n",
    "            try:\n",
    "                page_num = element[\"metadata\"][\"page_number\"] - 1  # Convert to zero-based index\n",
    "                bbox_points = element[\"metadata\"][\"coordinates\"][\"points\"]\n",
    "\n",
    "                # Extract bounding box coordinates\n",
    "                x_min, y_min = map(int, bbox_points[0])  # Top-left\n",
    "                x_max, y_max = map(int, bbox_points[2])  # Bottom-right\n",
    "\n",
    "                if page_num < 0 or page_num >= len(images):\n",
    "                    print(f\"❌ Error: Page number {page_num + 1} out of range.\")\n",
    "                    continue\n",
    "\n",
    "                # Convert PIL image to OpenCV format properly\n",
    "                page_img = np.array(images[page_num])  # Convert PIL to NumPy array\n",
    "                page_img = cv2.cvtColor(page_img, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR for OpenCV\n",
    "\n",
    "                # Verify if the image is loaded correctly\n",
    "                if page_img is None or page_img.size == 0:\n",
    "                    print(f\"❌ Error: Page {page_num + 1} could not be read properly.\")\n",
    "                    continue\n",
    "\n",
    "                # Draw bounding box around the table\n",
    "                color = (0, 0, 255)  # Red color for annotation\n",
    "                thickness = 3  # Thickness of the bounding box\n",
    "                cv2.rectangle(page_img, (x_min, y_min), (x_max, y_max), color, thickness)\n",
    "\n",
    "                # Put a label near the table\n",
    "                label_text = f\"Table {element['element_id']}\"\n",
    "                cv2.putText(\n",
    "                    page_img, label_text, (x_min, y_min - 10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2\n",
    "                )\n",
    "\n",
    "                # Save the annotated image\n",
    "                save_path = os.path.join(save_dir, f\"annotated_page_{page_num + 1}.png\")\n",
    "                cv2.imwrite(save_path, page_img)\n",
    "\n",
    "                print(f\"✅ Annotated table on page {page_num + 1}, saved to: {save_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing table {element['element_id']}: {e}\")\n",
    "\n",
    "# Example Usage\n",
    "pdf_path = \"CaseStudies.pdf\"  # Path to the PDF file\n",
    "annotate_tables(pdf_path, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop tables and Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_blocks(layout, image_path, output_dir):\n",
    "\n",
    "  # Create the output directory if it doesn't exist\n",
    "  if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "  im = cv2.imread(image_path)\n",
    "  block_count = 0\n",
    "\n",
    "  for l in layout:\n",
    "    if l.type == 'Table' or 'Text':\n",
    "      x_1 = round(l.block.x_1)\n",
    "      y_1 = round(l.block.y_1)\n",
    "      x_2 = round(l.block.x_2)\n",
    "      y_2 = round(l.block.y_2)\n",
    "\n",
    "      block_count += 1\n",
    "      output_filename = f\"{output_dir}/block_{block_count}.png\"\n",
    "      cv2.imwrite(output_filename, im[y_1:y_2, x_1:x_2])\n",
    "      print(f\"Block {block_count} saved to {output_filename}\")  # Print the message\n",
    "\n",
    "# Example usage:\n",
    "img_path = \"OUTPUTS\\output_images\\page_2.png\"\n",
    "extract_blocks(layout, img_path, 'TESTING\\Extracted Tables Pub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [el for el in elements if el.category == \"Table\"]\n",
    "\n",
    "print(tables[0].text)\n",
    "print(tables[0].metadata.text_as_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[5].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_html = tables[3].metadata.text_as_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert HTML table to pandas DataFrame\n",
    "dfs = pd.read_html(table_html)\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming there's only one table, get the DataFrame\n",
    "df = dfs[0]\n",
    "\n",
    "# Now you have the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
