{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r \"Requirements/Pbr_requirements.txt\" # Plumber wale k liye hai ye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r \"Requirements/crop_requirments.txt\" \n",
    "# %pip install -r \"Requirements/recog_requirments.txt\"  \n",
    "# %pip install -r \"Requirements/to_Img_requirments.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation Function to Page wise Image Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation Function to Cropping Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "installations for Function to Recognise and format tables as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install paddlepaddle paddleocr tensorflow opencv-python pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import pdfplumber\n",
    "import pandas as pd\n",
    "# import fitz\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from paddleocr import PaddleOCR\n",
    "#from unstructured.partition.image import partition_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"INPUTS/PDFs\"\n",
    "output_dir = \"RESULTS\"\n",
    "deb_output_dir = \"DEBUG_IMGs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyMuPdf Image Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(input_dir):\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        # Build the full path to the PDF file\n",
    "        pdf_path = os.path.join(input_dir, filename)\n",
    "        # Extract the base name (e.g., 'document' from 'document.pdf')\n",
    "        pdf_name = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Create the corresponding output directory\n",
    "        output_folder = f\"RESULTS/IMAGES/{pdf_name}-images\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        # Open the PDF file\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        print(f\"Processing PDF: {pdf_path}\")\n",
    "\n",
    "        # Iterate over each page in the PDF\n",
    "        for page_number in range(pdf_document.page_count):\n",
    "            page = pdf_document[page_number]\n",
    "            images = page.get_images(full=True)  # Get all images on the page\n",
    "            print(f\"  Page {page_number + 1} has {len(images)} image(s).\")\n",
    "\n",
    "            # Extract and save each image\n",
    "            for img_index, img in enumerate(images):\n",
    "                xref = img[0]  # Reference number of the image\n",
    "                base_image = pdf_document.extract_image(xref)  # Extract image data\n",
    "                image_bytes = base_image[\"image\"]  # Raw image bytes\n",
    "\n",
    "                # Construct a filename for each image\n",
    "                image_path = os.path.join(\n",
    "                    output_folder,\n",
    "                    f\"Page_{page_number + 1}_Image_{img_index + 1}.png\"\n",
    "                )\n",
    "\n",
    "                # Save the image to a file\n",
    "                with open(image_path, \"wb\") as img_file:\n",
    "                    img_file.write(image_bytes)\n",
    "                print(f\"    Saved: {image_path}\")\n",
    "\n",
    "        # Close the PDF document\n",
    "        pdf_document.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pdfplumber Tables Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = [f for f in os.listdir(input_dir) if f.endswith(\".pdf\")] # Get a list of all PDF files\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    input_path = os.path.join(input_dir, pdf_file)  # Full path to the current PDF\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_file))[0]  # Extract PDF name without extension\n",
    "    \n",
    "    output_base_dir1 = f\"TABLES/{pdf_name}-tables\"  # Tables output directory\n",
    "    debug_output_dir1 = f\"{pdf_name}-Markings\"  # Debug output directory\n",
    "    output_base_dir = os.path.join(output_dir, output_base_dir1)\n",
    "    debug_output_dir = os.path.join(deb_output_dir, debug_output_dir1)\n",
    "    \n",
    "    os.makedirs(output_base_dir, exist_ok=True)\n",
    "    os.makedirs(debug_output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Processing file: {pdf_file}\")\n",
    "    \n",
    "    with pdfplumber.open(input_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages, start=1):\n",
    "            print(f\"Processing Page {i} of {pdf_file}\")\n",
    "            \n",
    "            # Debug table detection visually\n",
    "            debug_image = page.to_image()\n",
    "            debug_image.draw_rects(page.debug_tablefinder().edges)\n",
    "            debug_image_path = os.path.join(debug_output_dir, f\"Page_{i}_debug.png\")\n",
    "            debug_image.save(debug_image_path)\n",
    "            print(f\"Debug image for Page {i} saved to {debug_image_path}\")\n",
    "            \n",
    "            # Extract tables from the page\n",
    "            tables = page.extract_tables()\n",
    "            if tables:\n",
    "                for j, table in enumerate(tables, start=1):\n",
    "                    df = pd.DataFrame(table[1:], columns=table[0])  # Assuming first row as header\n",
    "                    output_csv = os.path.join(output_base_dir, f\"Page_{i}_Table_{j}.csv\") #Output file path for each table\n",
    "                    df.to_csv(output_csv, index=False)  # Save the DataFrame to a CSV file\n",
    "                    print(f\"Table {j} from Page {i} saved to {output_csv}\")\n",
    "            else:\n",
    "                print(f\"No tables found on Page {i} of {pdf_file}.\")\n",
    "\n",
    "print(\"All files have been successfully processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pdfplumber Page wise Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdf_file in os.listdir(input_dir):\n",
    "    if pdf_file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(input_dir, pdf_file)\n",
    "        pdf_name = os.path.splitext(pdf_file)[0]  # Get the PDF file name without extension\n",
    "\n",
    "        text_output_folder = f\"RESULTS/PAGE_TEXTS/{pdf_name}-texts\"\n",
    "        image_output_folder = f\"DEBUG_IMGs/{pdf_name}-annotations\"\n",
    "        os.makedirs(text_output_folder, exist_ok=True)\n",
    "        os.makedirs(image_output_folder, exist_ok=True)\n",
    "\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "\n",
    "            for page_number, page in enumerate(pdf.pages):\n",
    "                # Extract table bounding boxes\n",
    "                tables = page.find_tables()\n",
    "                table_bboxes = [table.bbox for table in tables]\n",
    "\n",
    "                # Extract all characters with positions\n",
    "                characters = page.chars\n",
    "\n",
    "                # Filter out characters that overlap with table bounding boxes\n",
    "                non_table_chars = [\n",
    "                    char for char in characters\n",
    "                    if not any(\n",
    "                        bbox[0] <= float(char[\"x0\"]) <= bbox[2]\n",
    "                        and bbox[1] <= float(char[\"top\"]) <= bbox[3]\n",
    "                        for bbox in table_bboxes\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "                # Sort characters by their y-coordinate (top) and x-coordinate (left-to-right)\n",
    "                non_table_chars.sort(key=lambda c: (float(c[\"top\"]), float(c[\"x0\"])))\n",
    "\n",
    "                # Reconstruct the text layout, character by character\n",
    "                reconstructed_text = \"\"\n",
    "                last_top = None\n",
    "                last_x1 = None\n",
    "                line_buffer = []\n",
    "\n",
    "                # Thresholds\n",
    "                new_line_threshold = 5   # vertical distance to consider it a new line\n",
    "                space_gap_threshold = 1  # horizontal gap to consider adding a space\n",
    "\n",
    "                for char in non_table_chars:\n",
    "                    current_top = float(char[\"top\"])\n",
    "                    current_x0 = float(char[\"x0\"])\n",
    "                    current_x1 = float(char[\"x1\"])\n",
    "                    text_char = char[\"text\"]\n",
    "\n",
    "                    # Detect new line based on the vertical position\n",
    "                    if last_top is None or abs(current_top - last_top) > new_line_threshold:\n",
    "                        # Finish the existing line buffer\n",
    "                        if line_buffer:\n",
    "                            reconstructed_text += \"\".join(line_buffer) + \"\\n\"\n",
    "                        # Start a new line buffer with the current character\n",
    "                        line_buffer = [text_char]\n",
    "                        last_top = current_top\n",
    "                        last_x1 = current_x1\n",
    "                    else:\n",
    "                        # Same line, decide if we need to add a space for a big gap\n",
    "                        if last_x1 is not None:\n",
    "                            gap = current_x0 - last_x1\n",
    "\n",
    "                            # If gap is large enough and the next character isn't an explicit space,\n",
    "                            # add a single space (but do NOT double-space if we already ended with a space)\n",
    "                            if gap > space_gap_threshold and text_char not in [\" \", \"\\t\"]:\n",
    "                                if not line_buffer or (line_buffer and line_buffer[-1] != \" \"):\n",
    "                                    line_buffer.append(\" \")\n",
    "\n",
    "                        # Now add the actual character\n",
    "                        line_buffer.append(text_char)\n",
    "                        last_x1 = current_x1\n",
    "                        last_top = current_top\n",
    "\n",
    "                # Add the final line to the reconstructed text\n",
    "                if line_buffer:\n",
    "                    reconstructed_text += \"\".join(line_buffer) + \"\\n\"\n",
    "\n",
    "                # Save the reconstructed text to a file\n",
    "                text_file_path = os.path.join(text_output_folder, f\"page{page_number + 1}.txt\")\n",
    "                with open(text_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "                    text_file.write(reconstructed_text)\n",
    "\n",
    "                print(f\"Character-level text with spacing from Page {page_number + 1} saved to: {text_file_path}\")\n",
    "\n",
    "                # Annotate the remaining characters (non-table chars)\n",
    "                page_image = page.to_image()\n",
    "                annotated_image = page_image.reset().draw_rects([\n",
    "                    {\n",
    "                        \"x0\": float(char[\"x0\"]),\n",
    "                        \"x1\": float(char[\"x1\"]),\n",
    "                        \"top\": float(char[\"top\"]),\n",
    "                        \"bottom\": float(char[\"bottom\"]),\n",
    "                    }\n",
    "                    for char in non_table_chars\n",
    "                ])\n",
    "\n",
    "                # Save the annotated image\n",
    "                annotated_image_path = os.path.join(image_output_folder, f\"page_{page_number + 1}_annotated.png\")\n",
    "                annotated_image.save(annotated_image_path)\n",
    "                print(f\"Annotated image saved to: {annotated_image_path}\")\n",
    "\n",
    "        print(f\"Processing complete for {pdf_name}. Check the folders for results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unstructured Tables Extraction (OCR Based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Page wise Image Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_pages(input_dir, output_base_folder=\"INPUTS\\PDF IMAGEs\\Pages\", dpi=300):\n",
    "\n",
    "\n",
    "    os.makedirs(output_base_folder, exist_ok=True)\n",
    "    pdf_files = [f for f in os.listdir(input_dir) if f.lower().endswith('.pdf')]\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(input_dir, pdf_file)\n",
    "        pdf_name = os.path.splitext(pdf_file)[0]\n",
    "        pdf_output_folder = os.path.join(output_base_folder, f\"{pdf_name}-Pages\")\n",
    "        os.makedirs(pdf_output_folder, exist_ok=True)\n",
    "\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        page_count = pdf_document.page_count\n",
    "        print(f\"Processing '{pdf_file}' ({page_count} pages)...\")\n",
    "\n",
    "        for page_number in range(page_count):\n",
    "            page = pdf_document[page_number]\n",
    "            pix = page.get_pixmap(dpi=dpi, colorspace=fitz.csRGB)  # Render at specified DPI in RGB\n",
    "            image_path = os.path.join(pdf_output_folder, f'page_{page_number + 1}.png')\n",
    "            pix.save(image_path)\n",
    "            print(f'Saved: {image_path}')\n",
    "\n",
    "        pdf_document.close()\n",
    "        print(f\"Finished processing '{pdf_file}'! Pages saved in '{pdf_output_folder}'\\n\")\n",
    "\n",
    "    print(\"\\n All PDFs processed successfully! Extracted pages are stored in '{output_base_folder}'.\")\n",
    "\n",
    "extract_pdf_pages(input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Cropping Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unstructured.partition.image import partition_image\n",
    "\n",
    "def extract_and_crop_tables(input_base_folder, output_base_folder=\"INPUTS\\PDF IMAGEs\\Cropped Tables\", top_left_padding=5, bottom_right_padding=7):\n",
    "\n",
    "    os.makedirs(output_base_folder, exist_ok=True)\n",
    "\n",
    "    subfolders = [f for f in os.listdir(input_base_folder) if os.path.isdir(os.path.join(input_base_folder, f))]\n",
    "\n",
    "    for subfolder in subfolders:\n",
    "        input_folder = os.path.join(input_base_folder, subfolder)\n",
    "        output_folder = os.path.join(output_base_folder, f\"{subfolder}-Cropped\")\n",
    "\n",
    "        # Ensure the output folder exists for each subfolder\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        # Function to crop an image with padding and save it\n",
    "        def crop_with_padding(image_path, coordinates, output_folder, filename):\n",
    "            \"\"\"Crop the specified coordinates from the image, apply padding, and save it.\"\"\"\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Error loading image: {image_path}\")\n",
    "                return\n",
    "\n",
    "            x_min = int(min(pt[0] for pt in coordinates))\n",
    "            y_min = int(min(pt[1] for pt in coordinates))\n",
    "            x_max = int(max(pt[0] for pt in coordinates))\n",
    "            y_max = int(max(pt[1] for pt in coordinates))\n",
    "\n",
    "            # Apply top-left and bottom-right padding\n",
    "            x_min_padded = max(0, x_min - top_left_padding)\n",
    "            y_min_padded = max(0, y_min - top_left_padding)\n",
    "            x_max_padded = min(image.shape[1], x_max + bottom_right_padding * 2)\n",
    "            y_max_padded = min(image.shape[0], y_max + bottom_right_padding)\n",
    "\n",
    "            # Crop the image\n",
    "            cropped_image = image[y_min_padded:y_max_padded, x_min_padded:x_max_padded]\n",
    "\n",
    "            # Save cropped image\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            cv2.imwrite(output_path, cropped_image)\n",
    "            print(f\"Cropped image saved to: {output_path}\")\n",
    "\n",
    "        # Dictionary to count tables per image\n",
    "        table_counter = {}\n",
    "\n",
    "        # Get all image files in the input folder\n",
    "        image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "        # Process each image file\n",
    "        for image_file in image_files:\n",
    "            image_path = os.path.join(input_folder, image_file)\n",
    "            image_name = os.path.splitext(image_file)[0]  # Extract image name without extension\n",
    "\n",
    "            # Perform table detection using Unstructured's partition_image\n",
    "            elements = partition_image(filename=image_path, infer_table_structure=True, strategy='hi_res')\n",
    "\n",
    "            # Convert elements to JSON structure\n",
    "            element_dict = [el.to_dict() for el in elements]\n",
    "\n",
    "            # Extract and crop tables\n",
    "            for item in element_dict:\n",
    "                if isinstance(item, dict) and item.get(\"type\") == \"Table\":\n",
    "                    try:\n",
    "                        coordinates = item[\"metadata\"][\"coordinates\"][\"points\"]\n",
    "\n",
    "                        # Count tables per image\n",
    "                        table_counter[image_name] = table_counter.get(image_name, 0) + 1\n",
    "                        table_number = table_counter[image_name]\n",
    "\n",
    "                        # Set output filename for cropped table image\n",
    "                        filename = f\"{image_name}_Table_{table_number}.png\"\n",
    "\n",
    "                        # Crop and save the table\n",
    "                        crop_with_padding(image_path, coordinates, output_folder, filename)\n",
    "\n",
    "                    except KeyError as e:\n",
    "                        print(f\"Missing key {e} in item: {item}\")\n",
    "\n",
    "        print(f\"\\n Finished processing folder: {subfolder}! Cropped tables saved in: {output_folder}\")\n",
    "\n",
    "    print(\"\\n All image folders processed successfully! Cropped tables are stored in:\", output_base_folder)\n",
    "\n",
    "input_directory = \"INPUTS\\PDF IMAGEs\\Pages\"\n",
    "output_directory = \"INPUTS\\PDF IMAGEs\\Cropped Tables\"\n",
    "\n",
    "extract_and_crop_tables(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Recognise and format tables as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables_from_images(image_folder, output_folder=\"Extracted Tables\"):\n",
    "\n",
    "    ocr = PaddleOCR(lang='en')\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Function to compute intersection of two bounding boxes\n",
    "    def intersection(box_1, box_2):\n",
    "        return [box_2[0], box_1[1], box_2[2], box_1[3]]\n",
    "\n",
    "    # Function to compute IoU (Intersection over Union)\n",
    "    def iou(box_1, box_2):\n",
    "        x_1 = max(box_1[0], box_2[0])\n",
    "        y_1 = max(box_1[1], box_2[1])\n",
    "        x_2 = min(box_1[2], box_2[2])\n",
    "        y_2 = min(box_1[3], box_2[3])\n",
    "\n",
    "        inter = abs(max((x_2 - x_1, 0)) * max((y_2 - y_1), 0))\n",
    "        if inter == 0:\n",
    "            return 0\n",
    "\n",
    "        box_1_area = abs((box_1[2] - box_1[0]) * (box_1[3] - box_1[1]))\n",
    "        box_2_area = abs((box_2[2] - box_2[0]) * (box_2[3] - box_2[1]))\n",
    "\n",
    "        return inter / float(box_1_area + box_2_area - inter)\n",
    "\n",
    "    # Get all image files from the folder\n",
    "    image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    # Process each image\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        image_cv = cv2.imread(image_path)\n",
    "\n",
    "        # Ensure the image was loaded correctly\n",
    "        if image_cv is None:\n",
    "            print(f\"Error loading image: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        image_height, image_width = image_cv.shape[:2]\n",
    "\n",
    "        # Perform OCR\n",
    "        output = ocr.ocr(image_path)[0]\n",
    "\n",
    "        # Extract bounding boxes, detected text, and confidence scores\n",
    "        boxes = [line[0] for line in output]\n",
    "        texts = [line[1][0] for line in output]\n",
    "        probabilities = [line[1][1] for line in output]\n",
    "\n",
    "        # Copy image for processing\n",
    "        im = image_cv.copy()\n",
    "\n",
    "        horiz_boxes = []\n",
    "        vert_boxes = []\n",
    "\n",
    "        # Generate horizontal and vertical bounding boxes\n",
    "        for box in boxes:\n",
    "            x_h, x_v = 0, int(box[0][0])\n",
    "            y_h, y_v = int(box[0][1]), 0\n",
    "            width_h, width_v = image_width, int(box[2][0] - box[0][0])\n",
    "            height_h, height_v = int(box[2][1] - box[0][1]), image_height\n",
    "\n",
    "            horiz_boxes.append([x_h, y_h, x_h + width_h, y_h + height_h])\n",
    "            vert_boxes.append([x_v, y_v, x_v + width_v, y_v + height_v])\n",
    "\n",
    "            cv2.rectangle(im, (x_h, y_h), (x_h + width_h, y_h + height_h), (0, 0, 255), 1)\n",
    "            cv2.rectangle(im, (x_v, y_v), (x_v + width_v, y_v + height_v), (0, 255, 0), 1)\n",
    "\n",
    "        # Apply Non-Maximum Suppression (NMS) for horizontal boxes\n",
    "        horiz_out = tf.image.non_max_suppression(\n",
    "            horiz_boxes, probabilities, max_output_size=1000, iou_threshold=0.1, score_threshold=float('-inf')\n",
    "        )\n",
    "        horiz_lines = np.sort(np.array(horiz_out))\n",
    "\n",
    "        im_nms = image_cv.copy()\n",
    "\n",
    "        for val in horiz_lines:\n",
    "            cv2.rectangle(im_nms, (int(horiz_boxes[val][0]), int(horiz_boxes[val][1])),\n",
    "                          (int(horiz_boxes[val][2]), int(horiz_boxes[val][3])), (0, 0, 255), 1)\n",
    "\n",
    "        # Apply Non-Maximum Suppression (NMS) for vertical boxes\n",
    "        vert_out = tf.image.non_max_suppression(\n",
    "            vert_boxes, probabilities, max_output_size=1000, iou_threshold=0.1, score_threshold=float('-inf')\n",
    "        )\n",
    "        vert_lines = np.sort(np.array(vert_out))\n",
    "\n",
    "        for val in vert_lines:\n",
    "            cv2.rectangle(im_nms, (int(vert_boxes[val][0]), int(vert_boxes[val][1])),\n",
    "                          (int(vert_boxes[val][2]), int(vert_boxes[val][3])), (255, 0, 0), 1)\n",
    "\n",
    "        # Create an empty table structure\n",
    "        out_array = [[\"\" for _ in range(len(vert_lines))] for _ in range(len(horiz_lines))]\n",
    "\n",
    "        # Sort bounding boxes based on vertical position\n",
    "        unordered_boxes = [vert_boxes[i][0] for i in vert_lines]\n",
    "        ordered_boxes = np.argsort(unordered_boxes)\n",
    "\n",
    "        # Fill the table using intersection and IoU logic\n",
    "        for i in range(len(horiz_lines)):\n",
    "            for j in range(len(vert_lines)):\n",
    "                resultant = intersection(horiz_boxes[horiz_lines[i]], vert_boxes[vert_lines[ordered_boxes[j]]])\n",
    "\n",
    "                for b in range(len(boxes)):\n",
    "                    the_box = [boxes[b][0][0], boxes[b][0][1], boxes[b][2][0], boxes[b][2][1]]\n",
    "                    if iou(resultant, the_box) > 0.1:\n",
    "                        out_array[i][j] = texts[b]\n",
    "\n",
    "        # Convert to a structured array\n",
    "        out_array = np.array(out_array)\n",
    "\n",
    "        # Save extracted text and structure as a CSV file with the image filename\n",
    "        csv_filename = f\"{os.path.splitext(image_file)[0]}.csv\"\n",
    "        csv_output_path = os.path.join(output_folder, csv_filename)\n",
    "        pd.DataFrame(out_array).to_csv(csv_output_path, index=False, header=False)\n",
    "\n",
    "        print(f\"Processing completed for {image_file}. Results saved in {output_folder}\")\n",
    "\n",
    "    print(\"\\n All images processed successfully! Extracted tables saved in:\", output_folder)\n",
    "\n",
    "input_directory = \"PDF1 Cropped Tables\"\n",
    "output_directory = \"PDF1 Tables 1\"\n",
    "\n",
    "extract_tables_from_images(input_directory, output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
