3 Learning to Memorize at Test Time
this section, we present a neural long-term memory module, which is a meta models that learns to memorize at test time. In Section 3.1, we first discuss the motivation and the design of the neural memory. In Section 3.2, we discuss how our architecture design can benefit from a fast and parallelizable training. Finally, in Section 3.3, we augment EL overcome the lack of long-term memory and to enable the model to learn, forget, and retrieve information, in our architecture using persistent memory module, in which we use learnable but data-independent parameters to learn meta information about the task.
3.1 Long-term Memory
To design a neural long-term memory module, we need a model that can encode the abstraction of the past history into its parameters. An example of this can be LLMs that are shown to be memorizing their training data (Leybzon and Kervadec 2024; Schwarzschild et al. 2024; Staab et al. 2024). Therefore, a simple idea is to train a neural network and expect it to memorize its training data. Memorization, however, has almost always been known as an undesirable phenomena in neural networks as it limits the model generalization (Bayat et al. 2024), causes privacy concerns (Staab et al. 2024), and so results in poor performance at test time. Moreover, the memorization of the training data might not be helpful at test time, in which the data might be out-of-distribution. We argue that, we need an online meta-model that learns how to memorize/forget the data at test time. In this setup, the model is learning a function that is capable of memorization, but it is not overfitting to the training data, resulting in a better generalization at test time.
Learning Process and Surprise Metric. The key idea to train a long-term memory is to treat its training as an online learning problem, in which we aim to compress the past information x),...,x;-; into the parameters of our long-term neural memory module M;. As discussed earlier, an event that violates the expectations (ie., is surprising) is more memorable for humans (Mandler 2014). Inspired by this, a simple definition of surprise for a model can be its gradient with respect to the input. The larger the gradient is, the more different the input data is from the past data. Accordingly, using this surprise score, we can update the memory as:
Mi = Meas — 8 VE(Me-15 x4). (8) ——_———_
Surprise
This surprise metric, however, can result in missing important information that comes after a big surprising moment. That is, the gradient can become extremely small after several surprising steps, leading to stocking in a flat area (ie., local minima), and missing information about some parts of the sequence. From the human memory perspective, an event might not consistently surprise us through a long-period of time although it is memorable. The reason is that the initial moment is surprising enough to get our attention through a long time frame, leading to memorizing the entire time frame. To improve the above surprise metric (Equation 8), we break the surprise metric into (1) past surprise, which measures the surprise amount of a very recent past; and (2) momentary surprise, which measures the surprise of incoming data:
Mis = Me-1 + Si, (9)
Sp= Nt St-1 —0, Ve (M-1; Xt) : (10) “SS”
n———_J>_
Past Surprise Momentary Surprise
Interestingly, this formulation is similar to gradient descent with momentum, where S, is the momentum element. Therefore, the momentum here act as a memory of surprise across time (sequence length). In this formulation, the term 7; is a data-dependent surprise decay (a function of x;), controlling how surprise decays over time, and the term 6; is controlling how much of momentary surprise should be incorporated into the final surprise metric in a data-dependent manner. This data-dependency is particularly important in this design: While surprise of previous tokens might be needed to affect the surprise of the next token, it is mostly valid if all tokens are relevant and are in the same context. Accordingly, a data-dependent 7 can control if memory needs to: (1) ignore the last surprise by setting 7, — 0 (possibly due to the change of context), or (2) fully incorporate the last surprise by setting n+ — 1 (possibly as the token is highly relevant to its recent past tokens).
Objective. Our above surprise metric is based on a loss function ¢(.;.), which is the objective that our memory is learning to act as it at test time. That is, our memory module is a meta model that learns a function based on the loss function £(.;.).
