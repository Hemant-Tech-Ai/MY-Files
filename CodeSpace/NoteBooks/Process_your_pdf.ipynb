{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation Function to Page wise Image Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation Function to Cropping Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "installations for Function to Recognise and format tables as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install paddlepaddle paddleocr tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import fitz\n",
    "import cv2\n",
    "from unstructured.partition.image import partition_image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from paddleocr import PaddleOCR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"TEST INPUT PDF\"\n",
    "output_dir = \"TEST RESULT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined Process 1.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025/02/13 16:45:26] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\Hemant.Singhsidar/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\Hemant.Singhsidar/.paddleocr/whl\\\\rec\\\\en\\\\en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='c:\\\\Users\\\\Hemant.Singhsidar\\\\Desktop\\\\CodeSpace\\\\.env\\\\lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=False, cls_model_dir='C:\\\\Users\\\\Hemant.Singhsidar/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, formula_algorithm='LaTeXOCR', formula_model_dir=None, formula_char_dict_path=None, formula_batch_num=1, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, formula=False, ocr=True, recovery=False, recovery_to_markdown=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "Found 1 PDF files to process\n",
      "\n",
      "Processing PDF: 2501.00663v1.pdf\n",
      "  Page 1 has 0 embedded image(s).\n",
      "Processed page 1/27\n",
      "  Page 2 has 0 embedded image(s).\n",
      "Processed page 2/27\n",
      "  Page 3 has 0 embedded image(s).\n",
      "Processed page 3/27\n",
      "  Page 4 has 0 embedded image(s).\n",
      "Processed page 4/27\n",
      "  Page 5 has 0 embedded image(s).\n",
      "Processed page 5/27\n",
      "  Page 6 has 0 embedded image(s).\n",
      "Processed page 6/27\n",
      "  Page 7 has 1 embedded image(s).\n",
      "    Saved embedded image: TEST RESULT 1\\Extracted Images\\2501.00663v1-images\\Page_7_Image_1.png\n",
      "Processed page 7/27\n",
      "  Page 8 has 1 embedded image(s).\n",
      "    Saved embedded image: TEST RESULT 1\\Extracted Images\\2501.00663v1-images\\Page_8_Image_1.png\n",
      "Processed page 8/27\n",
      "  Page 9 has 2 embedded image(s).\n",
      "    Saved embedded image: TEST RESULT 1\\Extracted Images\\2501.00663v1-images\\Page_9_Image_1.png\n",
      "    Saved embedded image: TEST RESULT 1\\Extracted Images\\2501.00663v1-images\\Page_9_Image_2.png\n",
      "Processed page 9/27\n",
      "  Page 10 has 1 embedded image(s).\n",
      "    Saved embedded image: TEST RESULT 1\\Extracted Images\\2501.00663v1-images\\Page_10_Image_1.png\n",
      "Processed page 10/27\n",
      "  Page 11 has 1 embedded image(s).\n",
      "    Saved embedded image: TEST RESULT 1\\Extracted Images\\2501.00663v1-images\\Page_11_Image_1.png\n",
      "Processed page 11/27\n",
      "  Page 12 has 0 embedded image(s).\n",
      "Processed page 12/27\n",
      "  Page 13 has 0 embedded image(s).\n",
      "Cropped table saved to: TEST RESULT 1\\Extracted Tables\\2501.00663v1-Tables\\page_13_Table_1.png\n",
      "[2025/02/13 16:46:11] ppocr WARNING: Since the angle classifier is not initialized, it will not be used during the forward process\n",
      "[2025/02/13 16:46:11] ppocr DEBUG: dt_boxes num : 494, elapsed : 0.7693021297454834\n",
      "[2025/02/13 16:46:25] ppocr DEBUG: rec_res num  : 494, elapsed : 13.186527729034424\n",
      "Saved CSV: page_13_Table_1.csv in TEST RESULT 1\\Extracted Tables CSV\\2501.00663v1-csv\n",
      "Processed page 13/27\n",
      "  Page 14 has 2 embedded image(s).\n",
      "    Saved embedded image: TEST RESULT 1\\Extracted Images\\2501.00663v1-images\\Page_14_Image_1.png\n",
      "    Saved embedded image: TEST RESULT 1\\Extracted Images\\2501.00663v1-images\\Page_14_Image_2.png\n",
      "Processed page 14/27\n",
      "  Page 15 has 4 embedded image(s).\n",
      "    Saved embedded image: TEST RESULT 1\\Extracted Images\\2501.00663v1-images\\Page_15_Image_1.png\n",
      "    Saved embedded image: TEST RESULT 1\\Extracted Images\\2501.00663v1-images\\Page_15_Image_2.png\n",
      "    Saved embedded image: TEST RESULT 1\\Extracted Images\\2501.00663v1-images\\Page_15_Image_3.png\n",
      "    Saved embedded image: TEST RESULT 1\\Extracted Images\\2501.00663v1-images\\Page_15_Image_4.png\n",
      "Cropped table saved to: TEST RESULT 1\\Extracted Tables\\2501.00663v1-Tables\\page_15_Table_2.png\n",
      "[2025/02/13 16:46:33] ppocr WARNING: Since the angle classifier is not initialized, it will not be used during the forward process\n",
      "[2025/02/13 16:46:33] ppocr DEBUG: dt_boxes num : 160, elapsed : 0.3579995632171631\n",
      "[2025/02/13 16:46:38] ppocr DEBUG: rec_res num  : 160, elapsed : 4.736539363861084\n",
      "Saved CSV: page_15_Table_2.csv in TEST RESULT 1\\Extracted Tables CSV\\2501.00663v1-csv\n",
      "Processed page 15/27\n",
      "  Page 16 has 1 embedded image(s).\n",
      "    Saved embedded image: TEST RESULT 1\\Extracted Images\\2501.00663v1-images\\Page_16_Image_1.png\n",
      "Cropped table saved to: TEST RESULT 1\\Extracted Tables\\2501.00663v1-Tables\\page_16_Table_3.png\n",
      "[2025/02/13 16:46:43] ppocr WARNING: Since the angle classifier is not initialized, it will not be used during the forward process\n",
      "[2025/02/13 16:46:43] ppocr DEBUG: dt_boxes num : 54, elapsed : 0.3191707134246826\n",
      "[2025/02/13 16:46:45] ppocr DEBUG: rec_res num  : 54, elapsed : 1.8581418991088867\n",
      "Saved CSV: page_16_Table_3.csv in TEST RESULT 1\\Extracted Tables CSV\\2501.00663v1-csv\n",
      "Processed page 16/27\n",
      "  Page 17 has 0 embedded image(s).\n",
      "Cropped table saved to: TEST RESULT 1\\Extracted Tables\\2501.00663v1-Tables\\page_17_Table_4.png\n",
      "[2025/02/13 16:46:48] ppocr WARNING: Since the angle classifier is not initialized, it will not be used during the forward process\n",
      "[2025/02/13 16:46:49] ppocr DEBUG: dt_boxes num : 43, elapsed : 0.31899189949035645\n",
      "[2025/02/13 16:46:50] ppocr DEBUG: rec_res num  : 43, elapsed : 1.5290043354034424\n",
      "Saved CSV: page_17_Table_4.csv in TEST RESULT 1\\Extracted Tables CSV\\2501.00663v1-csv\n",
      "Processed page 17/27\n",
      "  Page 18 has 0 embedded image(s).\n",
      "Processed page 18/27\n",
      "  Page 19 has 0 embedded image(s).\n",
      "Processed page 19/27\n",
      "  Page 20 has 0 embedded image(s).\n",
      "Processed page 20/27\n",
      "  Page 21 has 0 embedded image(s).\n",
      "Processed page 21/27\n",
      "  Page 22 has 0 embedded image(s).\n",
      "Processed page 22/27\n",
      "  Page 23 has 0 embedded image(s).\n",
      "Processed page 23/27\n",
      "  Page 24 has 0 embedded image(s).\n",
      "Processed page 24/27\n",
      "  Page 25 has 0 embedded image(s).\n",
      "Processed page 25/27\n",
      "  Page 26 has 0 embedded image(s).\n",
      "Processed page 26/27\n",
      "  Page 27 has 0 embedded image(s).\n",
      "Processed page 27/27\n",
      "Finished processing PDF: 2501.00663v1.pdf\n",
      "\n",
      "Processing complete!\n",
      "Text extracted to: TEST RESULT 1\\Extracted Text\n",
      "Tables extracted to: TEST RESULT 1\\Extracted Tables\n",
      "Table CSVs saved to: TEST RESULT 1\\Extracted Tables CSV\n",
      "Embedded images extracted to: TEST RESULT 1\\Extracted Images\n"
     ]
    }
   ],
   "source": [
    "def process_pdf_documents_update_2(input_dir, output_base_dir=output_dir, dpi=300):\n",
    "    \"\"\"\n",
    "    Process PDFs to extract text, tables, embedded images, and create CSV files in a single pass.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Directory containing PDF files\n",
    "        output_base_dir (str): Base directory for outputs\n",
    "        dpi (int): DPI for PDF to image conversion\n",
    "    \"\"\"\n",
    "    # Initialize output directories\n",
    "    text_output_folder = os.path.join(output_base_dir, \"Extracted Text\")\n",
    "    tables_output_folder = os.path.join(output_base_dir, \"Extracted Tables\")\n",
    "    tables_csv_folder = os.path.join(output_base_dir, \"Extracted Tables CSV\")\n",
    "    images_output_folder = os.path.join(output_base_dir, \"Extracted Images\")\n",
    "    \n",
    "    for folder in [text_output_folder, tables_output_folder, tables_csv_folder, images_output_folder]:\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    def intersection(box_1, box_2):\n",
    "        \"\"\"Calculate intersection of two bounding boxes\"\"\"\n",
    "        return [box_2[0], box_1[1], box_2[2], box_1[3]]\n",
    "\n",
    "    def iou(box_1, box_2):\n",
    "        \"\"\"Calculate Intersection over Union of two boxes\"\"\"\n",
    "        x_1 = max(box_1[0], box_2[0])\n",
    "        y_1 = max(box_1[1], box_2[1])\n",
    "        x_2 = min(box_1[2], box_2[2])\n",
    "        y_2 = min(box_1[3], box_2[3])\n",
    "\n",
    "        inter = abs(max((x_2 - x_1, 0)) * max((y_2 - y_1), 0))\n",
    "        if inter == 0:\n",
    "            return 0\n",
    "\n",
    "        box_1_area = abs((box_1[2] - box_1[0]) * (box_1[3] - box_1[1]))\n",
    "        box_2_area = abs((box_2[2] - box_2[0]) * (box_2[3] - box_2[1]))\n",
    "\n",
    "        return inter / float(box_1_area + box_2_area - inter)\n",
    "\n",
    "    def extract_embedded_images(pdf_document, page, pdf_name, page_number):\n",
    "        \"\"\"Extract embedded images from a PDF page\"\"\"\n",
    "        images_folder = os.path.join(images_output_folder, f\"{pdf_name}-images\")\n",
    "        os.makedirs(images_folder, exist_ok=True)\n",
    "        \n",
    "        images = page.get_images(full=True)\n",
    "        print(f\"  Page {page_number + 1} has {len(images)} embedded image(s).\")\n",
    "\n",
    "        for img_index, img in enumerate(images):\n",
    "            xref = img[0]\n",
    "            base_image = pdf_document.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "\n",
    "            # Construct filename for the image\n",
    "            image_filename = f\"Page_{page_number + 1}_Image_{img_index + 1}.png\"\n",
    "            image_path = os.path.join(images_folder, image_filename)\n",
    "\n",
    "            # Save the image\n",
    "            with open(image_path, \"wb\") as img_file:\n",
    "                img_file.write(image_bytes)\n",
    "            print(f\"    Saved embedded image: {image_path}\")\n",
    "\n",
    "    def process_element(element, page_name, pdf_name, image_cv):\n",
    "        \"\"\"Process individual elements (text or table) from the page\"\"\"\n",
    "        if element.get(\"type\") == \"Table\":\n",
    "            try:\n",
    "                coordinates = element[\"metadata\"][\"coordinates\"][\"points\"]\n",
    "                table_folder = os.path.join(tables_output_folder, f\"{pdf_name}-Tables\")\n",
    "                csv_folder = os.path.join(tables_csv_folder, f\"{pdf_name}-csv\")\n",
    "                \n",
    "                # Create necessary folders\n",
    "                os.makedirs(table_folder, exist_ok=True)\n",
    "                os.makedirs(csv_folder, exist_ok=True)\n",
    "                \n",
    "                # Crop and save table image\n",
    "                x_min = int(min(pt[0] for pt in coordinates))\n",
    "                y_min = int(min(pt[1] for pt in coordinates))\n",
    "                x_max = int(max(pt[0] for pt in coordinates))\n",
    "                y_max = int(max(pt[1] for pt in coordinates))\n",
    "                \n",
    "                # Add padding\n",
    "                x_min = max(0, x_min - 5)\n",
    "                y_min = max(0, y_min - 5)\n",
    "                x_max = min(image_cv.shape[1], x_max + 14)\n",
    "                y_max = min(image_cv.shape[0], y_max + 7)\n",
    "                \n",
    "                cropped_table = image_cv[y_min:y_max, x_min:x_max]\n",
    "                table_filename = f\"{page_name}_Table_{len(os.listdir(table_folder)) + 1}.png\"\n",
    "                table_path = os.path.join(table_folder, table_filename)\n",
    "                cv2.imwrite(table_path, cropped_table)\n",
    "                print(f\"Cropped table saved to: {table_path}\")\n",
    "                \n",
    "                # Process table with OCR and restructure\n",
    "                output = ocr.ocr(table_path)[0]\n",
    "                if not output:\n",
    "                    print(f\"No OCR output for table: {table_filename}\")\n",
    "                    return\n",
    "\n",
    "                # Extract bounding boxes and text\n",
    "                boxes = [line[0] for line in output]\n",
    "                texts = [line[1][0] for line in output]\n",
    "                probabilities = [line[1][1] for line in output]\n",
    "\n",
    "                # Generate horizontal and vertical boxes\n",
    "                image_height, image_width = cropped_table.shape[:2]\n",
    "                horiz_boxes = []\n",
    "                vert_boxes = []\n",
    "\n",
    "                for box in boxes:\n",
    "                    x_h, x_v = 0, int(box[0][0])\n",
    "                    y_h, y_v = int(box[0][1]), 0\n",
    "                    width_h, width_v = image_width, int(box[2][0] - box[0][0])\n",
    "                    height_h, height_v = int(box[2][1] - box[0][1]), image_height\n",
    "\n",
    "                    horiz_boxes.append([x_h, y_h, x_h + width_h, y_h + height_h])\n",
    "                    vert_boxes.append([x_v, y_v, x_v + width_v, y_v + height_v])\n",
    "\n",
    "                # Apply NMS\n",
    "                horiz_out = tf.image.non_max_suppression(\n",
    "                    horiz_boxes, probabilities, max_output_size=1000, iou_threshold=0.1\n",
    "                )\n",
    "                vert_out = tf.image.non_max_suppression(\n",
    "                    vert_boxes, probabilities, max_output_size=1000, iou_threshold=0.1\n",
    "                )\n",
    "\n",
    "                horiz_lines = np.sort(np.array(horiz_out))\n",
    "                vert_lines = np.sort(np.array(vert_out))\n",
    "\n",
    "                # Create table structure\n",
    "                out_array = [[\"\" for _ in range(len(vert_lines))] for _ in range(len(horiz_lines))]\n",
    "                unordered_boxes = [vert_boxes[i][0] for i in vert_lines]\n",
    "                ordered_boxes = np.argsort(unordered_boxes)\n",
    "\n",
    "                # Fill table with text\n",
    "                for i in range(len(horiz_lines)):\n",
    "                    for j in range(len(vert_lines)):\n",
    "                        resultant = intersection(\n",
    "                            horiz_boxes[horiz_lines[i]], \n",
    "                            vert_boxes[vert_lines[ordered_boxes[j]]]\n",
    "                        )\n",
    "\n",
    "                        for b in range(len(boxes)):\n",
    "                            the_box = [boxes[b][0][0], boxes[b][0][1], boxes[b][2][0], boxes[b][2][1]]\n",
    "                            if iou(resultant, the_box) > 0.1:\n",
    "                                out_array[i][j] = texts[b]\n",
    "\n",
    "                # Save as CSV\n",
    "                csv_filename = f\"{os.path.splitext(table_filename)[0]}.csv\"\n",
    "                csv_path = os.path.join(csv_folder, csv_filename)\n",
    "                pd.DataFrame(out_array).to_csv(csv_path, index=False, header=False)\n",
    "                print(f\"Saved CSV: {csv_filename} in {csv_folder}\")\n",
    "                \n",
    "            except (KeyError, IndexError) as e:\n",
    "                print(f\"Error processing table: {e}\")\n",
    "                \n",
    "        elif element.get(\"type\") != \"Table\":\n",
    "            # Handle text element\n",
    "            text_content = element.get(\"text\", \"\")\n",
    "            if text_content:\n",
    "                text_folder = os.path.join(text_output_folder, f\"{pdf_name}-Texts\")\n",
    "                os.makedirs(text_folder, exist_ok=True)\n",
    "                text_filename = f\"{page_name}_text.txt\"\n",
    "                text_path = os.path.join(text_folder, text_filename)\n",
    "                \n",
    "                with open(text_path, \"a\", encoding=\"utf-8\") as text_file:\n",
    "                    text_file.write(text_content + \"\\n\")\n",
    "\n",
    "    # Initialize OCR\n",
    "    ocr = PaddleOCR(lang='en')\n",
    "    \n",
    "    # Process each PDF\n",
    "    pdf_files = [f for f in os.listdir(input_dir) if f.lower().endswith('.pdf')]\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(input_dir, pdf_file)\n",
    "        pdf_name = os.path.splitext(pdf_file)[0]\n",
    "        print(f\"\\nProcessing PDF: {pdf_file}\")\n",
    "        \n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        \n",
    "        for page_number in range(pdf_document.page_count):\n",
    "            page = pdf_document[page_number]\n",
    "            page_name = f'page_{page_number + 1}'\n",
    "            \n",
    "            # Extract embedded images first\n",
    "            extract_embedded_images(pdf_document, page, pdf_name, page_number)\n",
    "            \n",
    "            # Convert PDF page to image for text and table extraction\n",
    "            pix = page.get_pixmap(dpi=dpi, colorspace=fitz.csRGB)\n",
    "            img_array = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, 3)\n",
    "            image_cv = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Create temporary image file\n",
    "            temp_image_path = os.path.join(output_base_dir, \"temp_page.png\")\n",
    "            cv2.imwrite(temp_image_path, image_cv)\n",
    "            \n",
    "            try:\n",
    "                # Extract elements using Unstructured\n",
    "                elements = partition_image(filename=temp_image_path, \n",
    "                                        infer_table_structure=True, \n",
    "                                        strategy='hi_res')\n",
    "                element_dict = [el.to_dict() for el in elements]\n",
    "                \n",
    "                # Process each element\n",
    "                for element in element_dict:\n",
    "                    process_element(element, page_name, pdf_name, image_cv)\n",
    "                    \n",
    "            finally:\n",
    "                if os.path.exists(temp_image_path):\n",
    "                    os.remove(temp_image_path)\n",
    "            \n",
    "            print(f\"Processed page {page_number + 1}/{pdf_document.page_count}\")\n",
    "        \n",
    "        pdf_document.close()\n",
    "        print(f\"Finished processing PDF: {pdf_file}\")\n",
    "\n",
    "    print(\"\\nProcessing complete!\")\n",
    "    print(f\"Text extracted to: {text_output_folder}\")\n",
    "    print(f\"Tables extracted to: {tables_output_folder}\")\n",
    "    print(f\"Table CSVs saved to: {tables_csv_folder}\")\n",
    "    print(f\"Embedded images extracted to: {images_output_folder}\")\n",
    "\n",
    "# Example usage:\n",
    "input_directory = \"TEST INPUT PDF\"\n",
    "process_pdf_documents_update_2(input_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('PDF1 Tables/budget_speech-Pages-Cropped/page_40_Table_1.csv')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('PDF1 Tables/budget_speech-Pages-Cropped\\page_31_Table_1.csv')\n",
    "df2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
