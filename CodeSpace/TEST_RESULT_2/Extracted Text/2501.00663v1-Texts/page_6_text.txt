In this work, we focus on associative memory, in which we aim to store the past data as the pairs of keys and values. Given xz, similar to Transformers (Vaswani et al. 2017), we use two linear layers to project x; into a key and value:
k; = xtWk, Vi = xt+Wy, (11)
where Wx and Wy € R@»*4», Next, we expect our memory module to learn the associations between keys and values. To this end, we define the loss as follows:
€(My-13 x1) = |Mi-1 (ke) — vell3 (12)
By optimizing the above loss function in the inner-loop of our meta model (memory), the model learns how to memorize the mapping between keys and values at test time. Note that, similar to meta-learning models (Nichol 2018; Zintgraf et al. 2019), training of the memory is in the inner-loop, and so parameters Wx and Wy are hyperparameters in the above loss function. Accordingly, in the inner loop, we optimize M’s weights, while in the outer-loop, we optimize other parameters of the entire architecture.
Forgetting Mechanism. When dealing with very large sequences (e.g., millions of tokens), it is crucial to manage which past information should be forgotten—even with a deep or a very large matrix-valued memory. To this end, we use an adaptive forgetting mechanism that allows the memory to forget the information that is not needed anymore, resulting in better managing the memory’s limited capacity. That is, given the next token x;, we modify the update rule as:
M;z = (1 - a7)Mi-1 + Si, (13)
= eSt-1 — 6; VE (Mr-1; Xr), (14)
St
where a; € [0,1] is the gating mechanism that flexibly controls the memory; i.e., decides how much information should be forgotten. For example, it can update the memory without affecting the past abstraction by letting a; — 0, and can clear the entire memory by letting a; — 1. Later in this section, we show that this weight decay mechanism is closely related to the gating mechanism in modern RNNs (Dao and Gu 2024; Orvieto et al. 2023).
Memory Architecture. In this paper, we focus on simple MLPs with Ly, > 1 layers as the architecture of our long-term memory. The main reason behind this choice is that we want to focus on better motivating the design of the long-term memory and ways that it can be incorporated into an architecture. However, our formulation and architectural design opens a new research direction to design neural architectures that are more effective and efficient in memorization of data. Recently, there has been a promising line of work to design such architectures (Berges et al. 2024; Cetin et al. 2024; J. Zhang et al. 2024), which incorporating them into our framework (i.e., replacing simple MLPs with such architectures) can be an interesting future work.
When using vector-valued or matrix-valued memory (De et al. 2024; Orvieto et al. 2023; S. Yang, B. Wang, Shen, et al. 2024), the memory module is compressing the past data and fit it into a line. That is, from the meta learning or online learning perspective (Yu Sun et al. 2024), using a matrix-valued memory M = W ¢€ R4*4in is equivalent to optimize €(W;-1;x;) = ||W-1k; - wills, which is an online linear regression objective and so the optimal solution assumes the underlying dependency of historical data is linear. On the other hand, we argue that deep memory modules (i.e., Ly = 2). Aligning with the theoretical results that MLPs with at least two layers are strictly more expressive than linear models (Hornik, Stinchcombe, and White 1989), in Section 5.5, we show that deep memory modules are more effective in practice.
Retrieving a Memory. In the above, we discuss how one can design and train a long-term memory module that learns to memorize at test time. A key remaining question is: How one can retrieve information from the memory? We simply use the forward pass without weight update (i-e., inference) to retrieve a memory correspond to a query. Formally, given an input x;, we use a linear layer Wo to project the input, i.e., q; = x;Wo and retrieve the corresponding (or useful) information from the memory y; by:
yr = M*(qz). (15)
