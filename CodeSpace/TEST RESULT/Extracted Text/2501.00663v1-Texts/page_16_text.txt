Table 4: Downstream evaluation of pre-trained DNA models on GenomicsBenchmarks (GreSova et al. 2023). We report top-1 classification accuracy (%).
Ens.
replace its Mamba module with our neural memory. We report the results on common time series forecasting benchmark datasets—ETT, ECL, Traffic, and Weather (H. Zhou et al. 2021). The results are reported in Table 3. Our neural memory module is outperforming all baselines, including Mamba-based, linear-based, and Transformer-based architectures.
5.7. DNA Modeling
In order to understand the capability of Titans beyond natural language, we further evaluate the performance of our neural memory module on DNA modeling tasks. To this end, we evaluate pre-trained models on the downstream tasks in GenomicsBenchmarks (GreSova et al. 2023). We follow the same experimental setups from Nguyen et al. (2024), and re-use the reported results of baselines by Arora et al. (2024). The performance of Titans (LMM) and baselines are reported in Table 4. We find that LMM is competitive with state-of-the-art architectures across different downstream genomics tasks.
5.8 Efficiency
In this part, we compare the efficiency of our neural memory as wellas Titans, with state-of-the-art sequence models. The training throughput of models for different sequence length x batch size are reported in Figure 9. Comparing," recurrent models, including our neural memory module, we can see ourmemory module is slightly slower than Mamba2 and Gated DeltaNet, mainly due to: (1) having deep memory and more expressive transition process (memory update), and (2) highly optimized kernel in the implementation of Mamba2. Interestingly, Titans (MAL) are faster than baselines as well as the memory module. The main reason for this better throughput is the highly optimized kernel of Flash- Attention (Dao 2024), which is used for implementing SWA and full attention module in Titans.
, ~e- Gated DeltaNet —4- Titans (MAL) NN sD Mamba fe Tana (ac) , ~ SS ——] 340 *— tS $ $ bas t + ™ + = “30 2 2K AK quence Length ® 16K
Figure 9: Training throughput compari- son of Titans and baselines.
5.9 Ablation Study
Finally, we perform ablation studies on the different architectural choices in Titans. We consider our neural memory module as a base model and then changing one component at a time: (1) replacing deep memory with linear memory, removing (2) convolution, (3) momentum in the surprise measure, (4) weight decay (or forgot mechanism), and (5) persistent memory. The results are reported in Table 5. All components of neural memory design are positively contributing to its performance, where the greatest contribution comes from weight decay, momentum, convolution, and persistent memory, respectively.
The Effect of Architectural Design. To evaluate the effect of architecture design, we compare the performance of three represented variants of Titans in three aspects of (i) language modeling, (ii) commen-sense reasoning, and (iii) long context NIAH (BABILong) tasks. The results are reported in Table 5. We find that MAC and MAG have close performance in language modeling and common-sense reasoning tasks, while MAC achieve significantly better performance in long-context NIAH. Both of these models achieve better performance than MAL. These results along with Figure 9, show a trade-off between fast training and more expressive design.
16
Table 4: Downstream evaluation of pre-trained DNA models on GenomicsBenchmarks (GreSova et al. 2023). We report top-1 classification accuracy (%).
Ens.
replace its Mamba module with our neural memory. We report the results on common time series forecasting benchmark datasets—ETT, ECL, Traffic, and Weather (H. Zhou et al. 2021). The results are reported in Table 3. Our neural memory module is outperforming all baselines, including Mamba-based, linear-based, and Transformer-based architectures.
5.7. DNA Modeling
In order to understand the capability of Titans beyond natural language, we further evaluate the performance of our neural memory module on DNA modeling tasks. To this end, we evaluate pre-trained models on the downstream tasks in GenomicsBenchmarks (GreSova et al. 2023). We follow the same experimental setups from Nguyen et al. (2024), and re-use the reported results of baselines by Arora et al. (2024). The performance of Titans (LMM) and baselines are reported in Table 4. We find that LMM is competitive with state-of-the-art architectures across different downstream genomics tasks.
5.8 Efficiency
In this part, we compare the efficiency of our neural memory as wellas Titans, with state-of-the-art sequence models. The training throughput of models for different sequence length x batch size are reported in Figure 9. Comparing," recurrent models, including our neural memory module, we can see ourmemory module is slightly slower than Mamba2 and Gated DeltaNet, mainly due to: (1) having deep memory and more expressive transition process (memory update), and (2) highly optimized kernel in the implementation of Mamba2. Interestingly, Titans (MAL) are faster than baselines as well as the memory module. The main reason for this better throughput is the highly optimized kernel of Flash- Attention (Dao 2024), which is used for implementing SWA and full attention module in Titans.
, ~e- Gated DeltaNet —4- Titans (MAL) NN sD Mamba fe Tana (ac) , ~ SS ——] 340 *— tS $ $ bas t + ™ + = “30 2 2K AK quence Length ® 16K
Figure 9: Training throughput compari- son of Titans and baselines.
5.9 Ablation Study
Finally, we perform ablation studies on the different architectural choices in Titans. We consider our neural memory module as a base model and then changing one component at a time: (1) replacing deep memory with linear memory, removing (2) convolution, (3) momentum in the surprise measure, (4) weight decay (or forgot mechanism), and (5) persistent memory. The results are reported in Table 5. All components of neural memory design are positively contributing to its performance, where the greatest contribution comes from weight decay, momentum, convolution, and persistent memory, respectively.
The Effect of Architectural Design. To evaluate the effect of architecture design, we compare the performance of three represented variants of Titans in three aspects of (i) language modeling, (ii) commen-sense reasoning, and (iii) long context NIAH (BABILong) tasks. The results are reported in Table 5. We find that MAC and MAG have close performance in language modeling and common-sense reasoning tasks, while MAC achieve significantly better performance in long-context NIAH. Both of these models achieve better performance than MAL. These results along with Figure 9, show a trade-off between fast training and more expressive design.
16
