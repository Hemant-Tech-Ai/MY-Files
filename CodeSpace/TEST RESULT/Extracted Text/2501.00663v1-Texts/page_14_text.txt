Table 2: Performance of Titans and baselines on S-NIAH task from RULER benchmark. The best results among simple and hybrid models are highlighted.
Model S-NIAH-PK S-NIAH-N S-NIAH-W 2K 4K 8K 16K 2K 4K 8K 16K 2K 4K 8K 16K TTT 98.4 98.8 980 884 60.2 366 10.2 44 788 280 4.4 0.0 Mamba2 98.6 614 310 5.4 98.4 558 142 00 422 4.2 0.0 0.0 DeltaNet 96.8 98.8 986 71.4 47.2 154 128 54 462 20.0 1.6 0.0 Titans (LMM) 99.8 984 98.2 96.2 100.0 99.8 93.4 80.2 904 894 85.8 80.6 Titans (MAC) 99.2 988 99.0 984 996 98.2 97.6 974 98.2 98.2 95.6 95.2 Titans (MAG) 994 980 974 974 99.2 988 97.2 98.6 98.0 98.0 90.2 88.2 Titans (MAL) 98.8 986 988 97.8 998 981 968 964 98.0 97.4 92.0 90.4 —@ Llama3.1-70B —¥- GPT-4 —& RMT-FT —k— Titans (MAC) —¥— RecurrentGemma-9B - GPT-4 —@® Qwen2.5-72B —&- GPT4o-mini —te Titans (MAC)-FT —@- Mamba2.8B —&- Gemma-9B —@ GPT4o-mini + Llama3.1-8B + RAG ~—@- Mamba-FT —+- RWKV-6-7B ~® Llama3.1-8B 80 100 Poe © Ss " / a So Accuracy (%) rf Accuracy (%) 3.8 o} + 103 tot 105 108 103 10? 705 108 T Sequence Length Sequence Length (a) Few-shot Setup (b) Fine-Tuning Setup
Figure 6: Performance of Titans and baselines on BABILong benchmark. Titans (MAC) outperforms all baselines, including extremely large models, e.g., GPT4.
we can see a significant drop in performance when increasing the sequence length; (3) Compared to DeltaNet, although it is capable of removing memory using delta rule, it cannot erase the memory, lacking forgetting mechanism. Finally, As expected we can see on par or better results when using Titans variants, where the best results correspond to MAC.
5.4 BABILong Benchmark
In the previous section we discussed the results on a simple NIAH tasks where a single needle needs to be retrieved. Although Titans showed better performance compared to baselines, their true advantage over very long sequences is still hidden. To this end, in this section, we use a harder task from BABILong benchmark (Yuri Kuratov et al. 2024), in which the model needs to reason across facts distributed in extremely long documents. We follow the original experimental setup and training process in the benchmark. There are two settings: (1) Few-shot setting, in which we use large pre-trained models, and (2) fine-tuning setting, where we fine-tune the MAC variant of Titans to compare it with other fine-tuned baselines. The results for few-shot setting are reported in Figure 6a. In this setup, we can see Titans outperform all baselines—i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024), RecurrentGemma-9B (Botev et al. 2024), Gemma-9B (Team et al. 2024), Llama3.1-8B (Touvron et al. 2023), GPT-4, and GPT4o0-mini (Achiam et al. 2023). These results are achieved while Titans (MAC) is having much less number of parameters than baselines.
In the fine-tuning setup, we compare the small fine-tuned version of Titans (MAC) with: (i) the fine-tuned version of small models (almost the same number of parameters as Titans) such as Mamba (Gu and Dao 2024), RMT (Bulatov, Yury Kuratov, and Burtsev 2022), (ii) large models with Retrieval-Augmented Generation (RAG) (P. Lewis et al. 2020) such as Llama3.1- 8B (Touvron et al. 2023), and (iii) extremely large models such as GPT-4 (Achiam et al. 2023), GPT40-mini, Qwen2.5-72B (A. Yang et al. 2024), and Llama3.1-70B (Touvron et al. 2023). Baseline results are reported by (Yuri Kuratov et al. 2024). The results of Titans and baselines are reported in Figure 6b. Titans outperform all models even extremely large models like GPT4. Also, compared to Transformer-based with memory models like RMT, Titans show better performance mainly due to their powerful memory. That is, RMT compress the historical data into 16 size vector-valued memory, while Titans with in-context online memory learner are capable of encoding the past into the parameters of the model. Interestingly, even
14
Table 2: Performance of Titans and baselines on S-NIAH task from RULER benchmark. The best results among simple and hybrid models are highlighted.
Model S-NIAH-PK S-NIAH-N S-NIAH-W 2K 4K 8K 16K 2K 4K 8K 16K 2K 4K 8K 16K TTT 98.4 98.8 980 884 60.2 366 10.2 44 788 280 4.4 0.0 Mamba2 98.6 614 310 5.4 98.4 558 142 00 422 4.2 0.0 0.0 DeltaNet 96.8 98.8 986 71.4 47.2 154 128 54 462 20.0 1.6 0.0 Titans (LMM) 99.8 984 98.2 96.2 100.0 99.8 93.4 80.2 904 894 85.8 80.6 Titans (MAC) 99.2 988 99.0 984 996 98.2 97.6 974 98.2 98.2 95.6 95.2 Titans (MAG) 994 980 974 974 99.2 988 97.2 98.6 98.0 98.0 90.2 88.2 Titans (MAL) 98.8 986 988 97.8 998 981 968 964 98.0 97.4 92.0 90.4 —@ Llama3.1-70B —¥- GPT-4 —& RMT-FT —k— Titans (MAC) —¥— RecurrentGemma-9B - GPT-4 —@® Qwen2.5-72B —&- GPT4o-mini —te Titans (MAC)-FT —@- Mamba2.8B —&- Gemma-9B —@ GPT4o-mini + Llama3.1-8B + RAG ~—@- Mamba-FT —+- RWKV-6-7B ~® Llama3.1-8B 80 100 Poe © Ss " / a So Accuracy (%) rf Accuracy (%) 3.8 o} + 103 tot 105 108 103 10? 705 108 T Sequence Length Sequence Length (a) Few-shot Setup (b) Fine-Tuning Setup
Figure 6: Performance of Titans and baselines on BABILong benchmark. Titans (MAC) outperforms all baselines, including extremely large models, e.g., GPT4.
we can see a significant drop in performance when increasing the sequence length; (3) Compared to DeltaNet, although it is capable of removing memory using delta rule, it cannot erase the memory, lacking forgetting mechanism. Finally, As expected we can see on par or better results when using Titans variants, where the best results correspond to MAC.
5.4 BABILong Benchmark
In the previous section we discussed the results on a simple NIAH tasks where a single needle needs to be retrieved. Although Titans showed better performance compared to baselines, their true advantage over very long sequences is still hidden. To this end, in this section, we use a harder task from BABILong benchmark (Yuri Kuratov et al. 2024), in which the model needs to reason across facts distributed in extremely long documents. We follow the original experimental setup and training process in the benchmark. There are two settings: (1) Few-shot setting, in which we use large pre-trained models, and (2) fine-tuning setting, where we fine-tune the MAC variant of Titans to compare it with other fine-tuned baselines. The results for few-shot setting are reported in Figure 6a. In this setup, we can see Titans outperform all baselines—i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024), RecurrentGemma-9B (Botev et al. 2024), Gemma-9B (Team et al. 2024), Llama3.1-8B (Touvron et al. 2023), GPT-4, and GPT4o0-mini (Achiam et al. 2023). These results are achieved while Titans (MAC) is having much less number of parameters than baselines.
In the fine-tuning setup, we compare the small fine-tuned version of Titans (MAC) with: (i) the fine-tuned version of small models (almost the same number of parameters as Titans) such as Mamba (Gu and Dao 2024), RMT (Bulatov, Yury Kuratov, and Burtsev 2022), (ii) large models with Retrieval-Augmented Generation (RAG) (P. Lewis et al. 2020) such as Llama3.1- 8B (Touvron et al. 2023), and (iii) extremely large models such as GPT-4 (Achiam et al. 2023), GPT40-mini, Qwen2.5-72B (A. Yang et al. 2024), and Llama3.1-70B (Touvron et al. 2023). Baseline results are reported by (Yuri Kuratov et al. 2024). The results of Titans and baselines are reported in Figure 6b. Titans outperform all models even extremely large models like GPT4. Also, compared to Transformer-based with memory models like RMT, Titans show better performance mainly due to their powerful memory. That is, RMT compress the historical data into 16 size vector-valued memory, while Titans with in-context online memory learner are capable of encoding the past into the parameters of the model. Interestingly, even
14
