Linear Non-Linear Momentum Calculation Within-Chunk Cross-Chunk Weight Decay W/o Decay (WoX — X)XT W/ Decay ) QpBy(WoX — X)X* All gradients are pre-computed Coe * Parallel Associative f f Sum Global Kernel via cumsum via Gradient via Matmul
Figure 1: The illustration of how the training of neural memory can be done in parallel and using matmuls.
3.2 How to Parallelize the Long-term Memory Training
As discussed above, the design of our long-term memory module is equivalent to training a meta model by optimizing associative memory loss function €(M;—1;x+) = ||Mi+-1 (kz) — vill using gradient descent with momentum and weight decay. Therefore, in theory, the training of long-term memory module requires O (N) FLOPs, where N is the sequence length. However, in practice, we need to parallelize the training process and to fully take advantage of hardware accelerators (e.g., TPUs, GPUs), we need to tensorize the process and use more matmuls.
Next, we show that calculating the weights in the inner loop with mini-batch gradient descent, data-dependent learning rate, and weight decay can be reformulated so that it uses only matmuls and sum. We build upon the work of Yu Sun et al. (2024) that shows forward pass of a model optimizing with the mini-batch gradient descent (with constant learning rate) can be calculated using matmuls. We can split the sequence into chunks of size b > 1, and write the mini-batch gradient descent as:
t M: = e _ ar )Mi-1 _ 0, VE(My-1; x1) = BrMo _ » FEV Mux), (16) i=1
where t’ = t — mod(t, b), and f; = ‘(1 — a;). For the sake of simplicity, we focus on the first chunk, i.e., t = b and so t’ = 0. Also, we explain the process for the case that M; = W, is linear. The process for MLPs with N, > 2 is similar. Using our loss function, we have:
b Ve(Wo; xr) = (Woxr - x1)x/) => » 0, PVE Wax) = O,By(WoX — X)X", (17) i=l i
where ©, = diag ({: O02... 05) and By is defined analogously on Bs, Note that, we do not need to store all Ox, and Bxp for k = 1,..., N/b, instead, we store these matrices for each chunk, resulting in using less memory. Next, we extend this representation so we can also incorporate the momentum term. In a chunk wise gradient descent with momentum, if we look at the momentum term, we have:
St = 4tSt—1 — 0; ur, (18)
where u; = Vé (Mr; xz). Note that, we can compute all u; at the same time, and so Equation 18 is a linear recurrence with u; as an input, S; as the hidden state, and 7; as input-dependent transition value. Accordingly, we can use parallel associative scan (J. T. Smith, Warrington, and Linderman 2023) to calculate S;s in this chunk.
Parameters as the Function of Chunks. Instead of making parameters like @;, 0;, and n; input-dependent (i.e., a function of token x;), we can make them functions of their chunk. Despite losing expressive power, this formulation can help to make the training even faster. In this case, we are using the same value for each of a, 0, and n in each chunk. Accordingly, in Equation 17, we can store © using a single scaler. Similarly we can make Equation 18 faster. That is, when 7 and @ are learnable but time-invariant inside each chunk, this equation becomes a linear time-invariant system (LTI), which can be computed by a global convolution (Gu, Goel, and Re 2022). In our experiments, we make these parameters as the functions of tokens. However, such simplifications (ie., as the function of chunks) can be the interest of future work to training larger models in more efficient manner.
Linear Non-Linear Momentum Calculation Within-Chunk Cross-Chunk Weight Decay W/o Decay (WoX — X)XT W/ Decay ) QpBy(WoX — X)X* All gradients are pre-computed Coe * Parallel Associative f f Sum Global Kernel via cumsum via Gradient via Matmul
Figure 1: The illustration of how the training of neural memory can be done in parallel and using matmuls.
3.2 How to Parallelize the Long-term Memory Training
As discussed above, the design of our long-term memory module is equivalent to training a meta model by optimizing associative memory loss function €(M;—1;x+) = ||Mi+-1 (kz) — vill using gradient descent with momentum and weight decay. Therefore, in theory, the training of long-term memory module requires O (N) FLOPs, where N is the sequence length. However, in practice, we need to parallelize the training process and to fully take advantage of hardware accelerators (e.g., TPUs, GPUs), we need to tensorize the process and use more matmuls.
Next, we show that calculating the weights in the inner loop with mini-batch gradient descent, data-dependent learning rate, and weight decay can be reformulated so that it uses only matmuls and sum. We build upon the work of Yu Sun et al. (2024) that shows forward pass of a model optimizing with the mini-batch gradient descent (with constant learning rate) can be calculated using matmuls. We can split the sequence into chunks of size b > 1, and write the mini-batch gradient descent as:
t M: = e _ ar )Mi-1 _ 0, VE(My-1; x1) = BrMo _ » FEV Mux), (16) i=1
where t’ = t — mod(t, b), and f; = ‘(1 — a;). For the sake of simplicity, we focus on the first chunk, i.e., t = b and so t’ = 0. Also, we explain the process for the case that M; = W, is linear. The process for MLPs with N, > 2 is similar. Using our loss function, we have:
b Ve(Wo; xr) = (Woxr - x1)x/) => » 0, PVE Wax) = O,By(WoX — X)X", (17) i=l i
where ©, = diag ({: O02... 05) and By is defined analogously on Bs, Note that, we do not need to store all Ox, and Bxp for k = 1,..., N/b, instead, we store these matrices for each chunk, resulting in using less memory. Next, we extend this representation so we can also incorporate the momentum term. In a chunk wise gradient descent with momentum, if we look at the momentum term, we have:
St = 4tSt—1 — 0; ur, (18)
where u; = Vé (Mr; xz). Note that, we can compute all u; at the same time, and so Equation 18 is a linear recurrence with u; as an input, S; as the hidden state, and 7; as input-dependent transition value. Accordingly, we can use parallel associative scan (J. T. Smith, Warrington, and Linderman 2023) to calculate S;s in this chunk.
Parameters as the Function of Chunks. Instead of making parameters like @;, 0;, and n; input-dependent (i.e., a function of token x;), we can make them functions of their chunk. Despite losing expressive power, this formulation can help to make the training even faster. In this case, we are using the same value for each of a, 0, and n in each chunk. Accordingly, in Equation 17, we can store © using a single scaler. Similarly we can make Equation 18 faster. That is, when 7 and @ are learnable but time-invariant inside each chunk, this equation becomes a linear time-invariant system (LTI), which can be computed by a global convolution (Gu, Goel, and Re 2022). In our experiments, we make these parameters as the functions of tokens. However, such simplifications (ie., as the function of chunks) can be the interest of future work to training larger models in more efficient manner.
