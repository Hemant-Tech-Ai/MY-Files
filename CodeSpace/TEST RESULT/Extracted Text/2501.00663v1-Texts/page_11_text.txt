Test Time Neural Memory Sb a Bey £8 ce| og cE gs F 8s 4 1S) wey 2 aan a4 ie) 04 1o) (6) bs} o Sequence A= 4 ab % B& % 4 g Learnable Data-Independent Weights 2 a o a4 rs ea (3) ee
Figure 5: Memory as a Layer (MAL) Architecture. In this architecture, the memory layer is responsible to compress the past and current context before the attention module.
where SW-Attn is sliding window attention. The main drawback of this design is that the power of the model is limited by each of the layers and so it cannot take advantage of the complementary data processing of attention and neural memory module. In our experiments, for evaluating memory in this design, we use a similar architecture as H3 (D. Y. Fu et al. 2023), where we replace the the sequence model with our neural memory module (LMM).
Memory Without Attention. Although in the above, we discussed MAL as the combination of LMMs and attention in a sequential manner, one simple variant of MAL is to treat LMM as a sequence model without any attention. From the memory perspective, as discussed in Section 1, we expect each part of the memory system to work independently, even if other components are disturbed. Therefore, a long-term memory module should still be a powerful model even without short-term memory (i.e., attention). We refer to this variant as LMM or Titans (LMM) in our experiments. We provide additional discussions on the connection of Titans and other modern recurrent models in Appendix C.
4.4 Architectural Details
For the sake of simplicity and presentation, we avoid discussing the implementation details like using residual connection, gating with linear layer, and normalization. In all blocks, we use residual connections. In our implementation, we use SiLU(.) activation (Elfwing, Uchibe, and Doya 2018) as the non-linear activation for computing query, key, and values and normalize queries and keys using f-norm.
Convolution. Following the recent modern linear recurrent models (Gu and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024), we incorporate a 1D depthwise-separable convolution layer after each of the query, key, and value projections. While not significantly affect the performance, these 1D convolutions have shown performance improvement and are also computationally efficient.
Gating. We also follow the recent architectures that use normalization and gating with a linear layer before the final output projection (Mehta et al. 2023).
Theorem 4.1. Contrary to Transformers, diagonal linear recurrent models, and DeltaNet, all of which are limited to TC° (Merrill, Petty, and Sabharwal 2024), Titans are capable of solving problems beyond TC °, meaning that Titans are theoretically more expressive than Transformers and most modern linear recurrent models in state tracking tasks.
5 Experiments
IWOs ext: we evaluate the performance of Titans and its variants in language modeling, commonsense reasoning, needle ) KN b in haystack, DNA modeling, and time series forecasting tasks!. In more details, in this section, we answer the ED. ll following empirical questions: (1) How do Titans perform compared to baselines in downstream tasks? (see §5.2,
ln the first version of the work, we aim to provide insights/evidences about why the learning paradigms of Titans are effective. We are working on finalizing the results of larger models and will report them in the next version.
11
Test Time Neural Memory Sb a Bey £8 ce| og cE gs F 8s 4 1S) wey 2 aan a4 ie) 04 1o) (6) bs} o Sequence A= 4 ab % B& % 4 g Learnable Data-Independent Weights 2 a o a4 rs ea (3) ee
Figure 5: Memory as a Layer (MAL) Architecture. In this architecture, the memory layer is responsible to compress the past and current context before the attention module.
where SW-Attn is sliding window attention. The main drawback of this design is that the power of the model is limited by each of the layers and so it cannot take advantage of the complementary data processing of attention and neural memory module. In our experiments, for evaluating memory in this design, we use a similar architecture as H3 (D. Y. Fu et al. 2023), where we replace the the sequence model with our neural memory module (LMM).
Memory Without Attention. Although in the above, we discussed MAL as the combination of LMMs and attention in a sequential manner, one simple variant of MAL is to treat LMM as a sequence model without any attention. From the memory perspective, as discussed in Section 1, we expect each part of the memory system to work independently, even if other components are disturbed. Therefore, a long-term memory module should still be a powerful model even without short-term memory (i.e., attention). We refer to this variant as LMM or Titans (LMM) in our experiments. We provide additional discussions on the connection of Titans and other modern recurrent models in Appendix C.
4.4 Architectural Details
For the sake of simplicity and presentation, we avoid discussing the implementation details like using residual connection, gating with linear layer, and normalization. In all blocks, we use residual connections. In our implementation, we use SiLU(.) activation (Elfwing, Uchibe, and Doya 2018) as the non-linear activation for computing query, key, and values and normalize queries and keys using f-norm.
Convolution. Following the recent modern linear recurrent models (Gu and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024), we incorporate a 1D depthwise-separable convolution layer after each of the query, key, and value projections. While not significantly affect the performance, these 1D convolutions have shown performance improvement and are also computationally efficient.
Gating. We also follow the recent architectures that use normalization and gating with a linear layer before the final output projection (Mehta et al. 2023).
Theorem 4.1. Contrary to Transformers, diagonal linear recurrent models, and DeltaNet, all of which are limited to TC° (Merrill, Petty, and Sabharwal 2024), Titans are capable of solving problems beyond TC °, meaning that Titans are theoretically more expressive than Transformers and most modern linear recurrent models in state tracking tasks.
5 Experiments
IWOs ext: we evaluate the performance of Titans and its variants in language modeling, commonsense reasoning, needle ) KN b in haystack, DNA modeling, and time series forecasting tasks!. In more details, in this section, we answer the ED. ll following empirical questions: (1) How do Titans perform compared to baselines in downstream tasks? (see §5.2,
ln the first version of the work, we aim to provide insights/evidences about why the learning paradigms of Titans are effective. We are working on finalizing the results of larger models and will report them in the next version.
11
