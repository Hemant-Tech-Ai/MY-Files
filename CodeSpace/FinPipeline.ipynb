{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation Function to Page wise Image Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation Function to Cropping Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "installations for Function to Recognise and format tables as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install paddlepaddle paddleocr tensorflow pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hemant.Singhsidar\\Desktop\\CodeSpace\\.env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import fitz\n",
    "import cv2\n",
    "from unstructured.partition.image import partition_image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from paddleocr import PaddleOCR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Warnings and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "logging.getLogger(\"ppocr\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directories(base_dir):\n",
    "    \"\"\"Setup output directories for the pipeline\"\"\"\n",
    "    directories = {\n",
    "        \"text\": os.path.join(base_dir, \"Extracted Text\"),\n",
    "        \"tables\": os.path.join(base_dir, \"Extracted Tables\"),\n",
    "        \"csv\": os.path.join(base_dir, \"Extracted Tables CSV\"),\n",
    "        \"images\": os.path.join(base_dir, \"Extracted Images\")\n",
    "    }\n",
    "    \n",
    "    for dir_path in directories.values():\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    return directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(box_1, box_2):\n",
    "    \"\"\"Calculate intersection of two bounding boxes\"\"\"\n",
    "    return [box_2[0], box_1[1], box_2[2], box_1[3]]\n",
    "\n",
    "def iou(box_1, box_2):\n",
    "    \"\"\"Calculate Intersection over Union of two boxes\"\"\"\n",
    "    x_1 = max(box_1[0], box_2[0])\n",
    "    y_1 = max(box_1[1], box_2[1])\n",
    "    x_2 = min(box_1[2], box_2[2])\n",
    "    y_2 = min(box_1[3], box_2[3])\n",
    "\n",
    "    inter = abs(max((x_2 - x_1, 0)) * max((y_2 - y_1), 0))\n",
    "    if inter == 0:\n",
    "        return 0\n",
    "\n",
    "    box_1_area = abs((box_1[2] - box_1[0]) * (box_1[3] - box_1[1]))\n",
    "    box_2_area = abs((box_2[2] - box_2[0]) * (box_2[3] - box_2[1]))\n",
    "\n",
    "    return inter / float(box_1_area + box_2_area - inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_table(element, page_name, pdf_name, image_cv, directories, ocr, padding):\n",
    "    \"\"\"Process and extract table from document\"\"\"\n",
    "    try:\n",
    "        coordinates = element[\"metadata\"][\"coordinates\"][\"points\"]\n",
    "        table_folder = os.path.join(directories[\"tables\"], f\"{pdf_name}-Tables\")\n",
    "        csv_folder = os.path.join(directories[\"csv\"], f\"{pdf_name}-csv\")\n",
    "        \n",
    "        os.makedirs(table_folder, exist_ok=True)\n",
    "        os.makedirs(csv_folder, exist_ok=True)\n",
    "        \n",
    "        # Extract table coordinates and apply padding\n",
    "        x_min = max(0, int(min(pt[0] for pt in coordinates)) - padding['left'])\n",
    "        y_min = max(0, int(min(pt[1] for pt in coordinates)) - padding['top'])\n",
    "        x_max = min(image_cv.shape[1], int(max(pt[0] for pt in coordinates)) + padding['right'])\n",
    "        y_max = min(image_cv.shape[0], int(max(pt[1] for pt in coordinates)) + padding['bottom'])\n",
    "        \n",
    "        # Process table image and save\n",
    "        cropped_table = image_cv[y_min:y_max, x_min:x_max]\n",
    "        table_filename = f\"{page_name}_Table_{len(os.listdir(table_folder)) + 1}.png\"\n",
    "        table_path = os.path.join(table_folder, table_filename)\n",
    "        cv2.imwrite(table_path, cropped_table)\n",
    "        \n",
    "        # OCR and structure extraction\n",
    "        return process_table_ocr(table_path, table_filename, csv_folder, ocr)\n",
    "        \n",
    "    except (KeyError, IndexError) as e:\n",
    "        print(f\"Error processing table: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_table_ocr(table_path, table_filename, csv_folder, ocr):\n",
    "    \"\"\"Process table with OCR and convert to structured format\"\"\"\n",
    "    output = ocr.ocr(table_path)[0]\n",
    "    if not output:\n",
    "        print(f\"No OCR output for table: {table_filename}\")\n",
    "        return None\n",
    "\n",
    "    # Extract text and structure\n",
    "    boxes = [line[0] for line in output]\n",
    "    texts = [line[1][0] for line in output]\n",
    "    probabilities = [line[1][1] for line in output]\n",
    "    \n",
    "    # Process and save structured data\n",
    "    structured_data = extract_table_structure(boxes, texts, probabilities)\n",
    "    if structured_data:\n",
    "        csv_filename = f\"{os.path.splitext(table_filename)[0]}.csv\"\n",
    "        csv_path = os.path.join(csv_folder, csv_filename)\n",
    "        pd.DataFrame(structured_data).to_csv(csv_path, index=False, header=False)\n",
    "        print(f\"Saved CSV: {csv_filename}\")\n",
    "    \n",
    "    return structured_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_documents(input_dir, output_base_dir=\"TEST RESULT7\", dpi=300,\n",
    "                         pad_left=5, pad_top=5, pad_right=14, pad_bottom=7):\n",
    "    \"\"\"\n",
    "    Main pipeline function to process PDF documents\n",
    "    \"\"\"\n",
    "    # Initialize directories and OCR\n",
    "    directories = setup_directories(output_base_dir)\n",
    "    ocr = PaddleOCR(lang='en')\n",
    "    padding = {'left': pad_left, 'top': pad_top, 'right': pad_right, 'bottom': pad_bottom}\n",
    "    \n",
    "    # Process PDFs\n",
    "    pdf_files = [f for f in os.listdir(input_dir) if f.lower().endswith('.pdf')]\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        process_single_pdf(pdf_file, input_dir, directories, ocr, padding, dpi)\n",
    "\n",
    "    print(\"\\nProcessing complete!\")\n",
    "    for dir_type, dir_path in directories.items():\n",
    "        print(f\"{dir_type.capitalize()} extracted to: {dir_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"TEST INPUT PDF\"\n",
    "    output_directory = \"TEST_RESULT_1\"\n",
    "    \n",
    "    # Use default padding\n",
    "    process_pdf_documents(input_directory, output_directory)\n",
    "    \n",
    "    # Or use custom padding\n",
    "    # process_pdf_documents(input_directory, output_directory,\n",
    "    #                      pad_left=10, pad_top=8, pad_right=16, pad_bottom=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hemant.Singhsidar\\Desktop\\CodeSpace\\.env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 PDF files to process\n",
      "\n",
      "Processing PDF: 2501.00663v1.pdf\n",
      "  Page 1 has 0 embedded image(s).\n",
      "Processed page 1/27\n",
      "  Page 2 has 0 embedded image(s).\n",
      "Processed page 2/27\n",
      "  Page 3 has 0 embedded image(s).\n",
      "Processed page 3/27\n",
      "  Page 4 has 0 embedded image(s).\n",
      "Processed page 4/27\n",
      "  Page 5 has 0 embedded image(s).\n",
      "Processed page 5/27\n",
      "  Page 6 has 0 embedded image(s).\n",
      "Processed page 6/27\n",
      "  Page 7 has 1 embedded image(s).\n",
      "    Saved embedded image: TEST_RESULT_2\\Extracted Images\\2501.00663v1-images\\Page_7_Image_1.png\n",
      "Processed page 7/27\n",
      "  Page 8 has 1 embedded image(s).\n",
      "    Saved embedded image: TEST_RESULT_2\\Extracted Images\\2501.00663v1-images\\Page_8_Image_1.png\n",
      "Processed page 8/27\n",
      "  Page 9 has 2 embedded image(s).\n",
      "    Saved embedded image: TEST_RESULT_2\\Extracted Images\\2501.00663v1-images\\Page_9_Image_1.png\n",
      "    Saved embedded image: TEST_RESULT_2\\Extracted Images\\2501.00663v1-images\\Page_9_Image_2.png\n",
      "Processed page 9/27\n",
      "  Page 10 has 1 embedded image(s).\n",
      "    Saved embedded image: TEST_RESULT_2\\Extracted Images\\2501.00663v1-images\\Page_10_Image_1.png\n",
      "Processed page 10/27\n",
      "  Page 11 has 1 embedded image(s).\n",
      "    Saved embedded image: TEST_RESULT_2\\Extracted Images\\2501.00663v1-images\\Page_11_Image_1.png\n",
      "Processed page 11/27\n",
      "  Page 12 has 0 embedded image(s).\n",
      "Processed page 12/27\n",
      "  Page 13 has 0 embedded image(s).\n",
      "Cropped table saved to: TEST_RESULT_2\\Extracted Tables\\2501.00663v1-Tables\\page_13_Table_1.png\n",
      "Saved CSV: page_13_Table_1.csv in TEST_RESULT_2\\Extracted Tables CSV\\2501.00663v1-csv\n",
      "Processed page 13/27\n",
      "  Page 14 has 2 embedded image(s).\n",
      "    Saved embedded image: TEST_RESULT_2\\Extracted Images\\2501.00663v1-images\\Page_14_Image_1.png\n",
      "    Saved embedded image: TEST_RESULT_2\\Extracted Images\\2501.00663v1-images\\Page_14_Image_2.png\n",
      "Processed page 14/27\n",
      "  Page 15 has 4 embedded image(s).\n",
      "    Saved embedded image: TEST_RESULT_2\\Extracted Images\\2501.00663v1-images\\Page_15_Image_1.png\n",
      "    Saved embedded image: TEST_RESULT_2\\Extracted Images\\2501.00663v1-images\\Page_15_Image_2.png\n",
      "    Saved embedded image: TEST_RESULT_2\\Extracted Images\\2501.00663v1-images\\Page_15_Image_3.png\n",
      "    Saved embedded image: TEST_RESULT_2\\Extracted Images\\2501.00663v1-images\\Page_15_Image_4.png\n",
      "Cropped table saved to: TEST_RESULT_2\\Extracted Tables\\2501.00663v1-Tables\\page_15_Table_2.png\n",
      "Saved CSV: page_15_Table_2.csv in TEST_RESULT_2\\Extracted Tables CSV\\2501.00663v1-csv\n",
      "Processed page 15/27\n",
      "  Page 16 has 1 embedded image(s).\n",
      "    Saved embedded image: TEST_RESULT_2\\Extracted Images\\2501.00663v1-images\\Page_16_Image_1.png\n",
      "Cropped table saved to: TEST_RESULT_2\\Extracted Tables\\2501.00663v1-Tables\\page_16_Table_3.png\n",
      "Saved CSV: page_16_Table_3.csv in TEST_RESULT_2\\Extracted Tables CSV\\2501.00663v1-csv\n",
      "Processed page 16/27\n",
      "  Page 17 has 0 embedded image(s).\n",
      "Cropped table saved to: TEST_RESULT_2\\Extracted Tables\\2501.00663v1-Tables\\page_17_Table_4.png\n",
      "Saved CSV: page_17_Table_4.csv in TEST_RESULT_2\\Extracted Tables CSV\\2501.00663v1-csv\n",
      "Processed page 17/27\n",
      "  Page 18 has 0 embedded image(s).\n",
      "Processed page 18/27\n",
      "  Page 19 has 0 embedded image(s).\n",
      "Processed page 19/27\n",
      "  Page 20 has 0 embedded image(s).\n",
      "Processed page 20/27\n",
      "  Page 21 has 0 embedded image(s).\n",
      "Processed page 21/27\n",
      "  Page 22 has 0 embedded image(s).\n",
      "Processed page 22/27\n",
      "  Page 23 has 0 embedded image(s).\n",
      "Processed page 23/27\n",
      "  Page 24 has 0 embedded image(s).\n",
      "Processed page 24/27\n",
      "  Page 25 has 0 embedded image(s).\n",
      "Processed page 25/27\n",
      "  Page 26 has 0 embedded image(s).\n",
      "Processed page 26/27\n",
      "  Page 27 has 0 embedded image(s).\n",
      "Processed page 27/27\n",
      "Finished processing PDF: 2501.00663v1.pdf\n",
      "\n",
      "Processing complete!\n",
      "Text extracted to: TEST_RESULT_2\\Extracted Text\n",
      "Tables extracted to: TEST_RESULT_2\\Extracted Tables\n",
      "Table CSVs saved to: TEST_RESULT_2\\Extracted Tables CSV\n",
      "Embedded images extracted to: TEST_RESULT_2\\Extracted Images\n"
     ]
    }
   ],
   "source": [
    "# 1. INSTALLATIONS\n",
    "#----------------\n",
    "\n",
    "# 1.1 PDF Processing Libraries\n",
    "# %pip install pymupdf\n",
    "\n",
    "# 1.2 Table Detection Libraries\n",
    "# %pip install \"unstructured[all-docs]\"\n",
    "\n",
    "# 1.3 OCR and Image Processing Libraries\n",
    "# %pip install paddlepaddle paddleocr tensorflow\n",
    "\n",
    "# 2. IMPORTS\n",
    "#-----------\n",
    "\n",
    "# 2.1 Basic Libraries\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# 2.2 Data Processing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 2.3 PDF and Image Processing\n",
    "import fitz\n",
    "import cv2\n",
    "from unstructured.partition.image import partition_image\n",
    "\n",
    "# 2.4 Deep Learning Libraries\n",
    "import tensorflow as tf\n",
    "from paddleocr import PaddleOCR\n",
    "\n",
    "# 2.5 Configure Warnings and Logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow warnings\n",
    "logging.getLogger(\"ppocr\").setLevel(logging.ERROR)  # Suppress PaddleOCR debug messages\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 3. DIRECTORY CONFIGURATION\n",
    "#--------------------------\n",
    "input_dir = \"TEST INPUT PDF\"\n",
    "output_dir = \"TEST_RESULT_2\"\n",
    "\n",
    "# 4. PROCESSING PARAMETERS\n",
    "#------------------------\n",
    "# 4.1 Default Parameters\n",
    "DEFAULT_DPI = 300\n",
    "DEFAULT_PADDING = {\n",
    "    'left': 5,\n",
    "    'top': 5,\n",
    "    'right': 14,\n",
    "    'bottom': 7\n",
    "}\n",
    "\n",
    "# 4.2 Custom Parameters (if needed)\n",
    "pad_left = 5\n",
    "pad_top = 5\n",
    "pad_right = 14\n",
    "pad_bottom = 7\n",
    "\n",
    "# 5. HELPER FUNCTIONS\n",
    "#-------------------\n",
    "def intersection(box_1, box_2):\n",
    "    \"\"\"Calculate intersection of two bounding boxes\"\"\"\n",
    "    return [box_2[0], box_1[1], box_2[2], box_1[3]]\n",
    "\n",
    "def iou(box_1, box_2):\n",
    "    \"\"\"Calculate Intersection over Union of two boxes\"\"\"\n",
    "    x_1 = max(box_1[0], box_2[0])\n",
    "    y_1 = max(box_1[1], box_2[1])\n",
    "    x_2 = min(box_1[2], box_2[2])\n",
    "    y_2 = min(box_1[3], box_2[3])\n",
    "\n",
    "    inter = abs(max((x_2 - x_1, 0)) * max((y_2 - y_1), 0))\n",
    "    if inter == 0:\n",
    "        return 0\n",
    "\n",
    "    box_1_area = abs((box_1[2] - box_1[0]) * (box_1[3] - box_1[1]))\n",
    "    box_2_area = abs((box_2[2] - box_2[0]) * (box_2[3] - box_2[1]))\n",
    "\n",
    "    return inter / float(box_1_area + box_2_area - inter)\n",
    "\n",
    "# 6. MAIN PROCESSING FUNCTION\n",
    "#---------------------------\n",
    "def process_pdf_documents_update(input_dir, output_base_dir=\"TEST RESULT7\", dpi=300,\n",
    "                               pad_left=5, pad_top=5, pad_right=14, pad_bottom=7):\n",
    "    \"\"\"\n",
    "    Process PDFs to extract text, tables, embedded images, and create CSV files in a single pass.\n",
    "    \"\"\"\n",
    "    # 6.1 Initialize Output Directories\n",
    "    text_output_folder = os.path.join(output_base_dir, \"Extracted Text\")\n",
    "    tables_output_folder = os.path.join(output_base_dir, \"Extracted Tables\")\n",
    "    tables_csv_folder = os.path.join(output_base_dir, \"Extracted Tables CSV\")\n",
    "    images_output_folder = os.path.join(output_base_dir, \"Extracted Images\")\n",
    "    \n",
    "    for folder in [text_output_folder, tables_output_folder, tables_csv_folder, images_output_folder]:\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # 6.2 Define Image Extraction Function\n",
    "    def extract_embedded_images(pdf_document, page, pdf_name, page_number):\n",
    "        images_folder = os.path.join(images_output_folder, f\"{pdf_name}-images\")\n",
    "        os.makedirs(images_folder, exist_ok=True)\n",
    "        \n",
    "        images = page.get_images(full=True)\n",
    "        print(f\"  Page {page_number + 1} has {len(images)} embedded image(s).\")\n",
    "\n",
    "        for img_index, img in enumerate(images):\n",
    "            xref = img[0]\n",
    "            base_image = pdf_document.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_filename = f\"Page_{page_number + 1}_Image_{img_index + 1}.png\"\n",
    "            image_path = os.path.join(images_folder, image_filename)\n",
    "            \n",
    "            with open(image_path, \"wb\") as img_file:\n",
    "                img_file.write(image_bytes)\n",
    "            print(f\"    Saved embedded image: {image_path}\")\n",
    "\n",
    "    # 6.3 Define Element Processing Function\n",
    "    def process_element(element, page_name, pdf_name, image_cv):\n",
    "        if element.get(\"type\") == \"Table\":\n",
    "            try:\n",
    "                coordinates = element[\"metadata\"][\"coordinates\"][\"points\"]\n",
    "                table_folder = os.path.join(tables_output_folder, f\"{pdf_name}-Tables\")\n",
    "                csv_folder = os.path.join(tables_csv_folder, f\"{pdf_name}-csv\")\n",
    "                \n",
    "                os.makedirs(table_folder, exist_ok=True)\n",
    "                os.makedirs(csv_folder, exist_ok=True)\n",
    "                \n",
    "                # Extract and pad coordinates\n",
    "                x_min = int(min(pt[0] for pt in coordinates))\n",
    "                y_min = int(min(pt[1] for pt in coordinates))\n",
    "                x_max = int(max(pt[0] for pt in coordinates))\n",
    "                y_max = int(max(pt[1] for pt in coordinates))\n",
    "                \n",
    "                x_min = max(0, x_min - pad_left)\n",
    "                y_min = max(0, y_min - pad_top)\n",
    "                x_max = min(image_cv.shape[1], x_max + pad_right)\n",
    "                y_max = min(image_cv.shape[0], y_max + pad_bottom)\n",
    "                \n",
    "                # Process table image\n",
    "                cropped_table = image_cv[y_min:y_max, x_min:x_max]\n",
    "                table_filename = f\"{page_name}_Table_{len(os.listdir(table_folder)) + 1}.png\"\n",
    "                table_path = os.path.join(table_folder, table_filename)\n",
    "                cv2.imwrite(table_path, cropped_table)\n",
    "                print(f\"Cropped table saved to: {table_path}\")\n",
    "                \n",
    "                # OCR Processing\n",
    "                output = ocr.ocr(table_path)[0]\n",
    "                if not output:\n",
    "                    print(f\"No OCR output for table: {table_filename}\")\n",
    "                    return\n",
    "\n",
    "                # Extract text and structure\n",
    "                boxes = [line[0] for line in output]\n",
    "                texts = [line[1][0] for line in output]\n",
    "                probabilities = [line[1][1] for line in output]\n",
    "\n",
    "                # Generate table structure\n",
    "                image_height, image_width = cropped_table.shape[:2]\n",
    "                horiz_boxes = []\n",
    "                vert_boxes = []\n",
    "\n",
    "                for box in boxes:\n",
    "                    x_h, x_v = 0, int(box[0][0])\n",
    "                    y_h, y_v = int(box[0][1]), 0\n",
    "                    width_h, width_v = image_width, int(box[2][0] - box[0][0])\n",
    "                    height_h, height_v = int(box[2][1] - box[0][1]), image_height\n",
    "\n",
    "                    horiz_boxes.append([x_h, y_h, x_h + width_h, y_h + height_h])\n",
    "                    vert_boxes.append([x_v, y_v, x_v + width_v, y_v + height_v])\n",
    "\n",
    "                # Apply NMS\n",
    "                horiz_out = tf.image.non_max_suppression(\n",
    "                    horiz_boxes, probabilities, max_output_size=1000, iou_threshold=0.1\n",
    "                )\n",
    "                vert_out = tf.image.non_max_suppression(\n",
    "                    vert_boxes, probabilities, max_output_size=1000, iou_threshold=0.1\n",
    "                )\n",
    "\n",
    "                horiz_lines = np.sort(np.array(horiz_out))\n",
    "                vert_lines = np.sort(np.array(vert_out))\n",
    "\n",
    "                # Create and fill table\n",
    "                out_array = [[\"\" for _ in range(len(vert_lines))] for _ in range(len(horiz_lines))]\n",
    "                unordered_boxes = [vert_boxes[i][0] for i in vert_lines]\n",
    "                ordered_boxes = np.argsort(unordered_boxes)\n",
    "\n",
    "                for i in range(len(horiz_lines)):\n",
    "                    for j in range(len(vert_lines)):\n",
    "                        resultant = intersection(\n",
    "                            horiz_boxes[horiz_lines[i]], \n",
    "                            vert_boxes[vert_lines[ordered_boxes[j]]]\n",
    "                        )\n",
    "\n",
    "                        for b in range(len(boxes)):\n",
    "                            the_box = [boxes[b][0][0], boxes[b][0][1], boxes[b][2][0], boxes[b][2][1]]\n",
    "                            if iou(resultant, the_box) > 0.1:\n",
    "                                out_array[i][j] = texts[b]\n",
    "\n",
    "                # Save CSV\n",
    "                csv_filename = f\"{os.path.splitext(table_filename)[0]}.csv\"\n",
    "                csv_path = os.path.join(csv_folder, csv_filename)\n",
    "                pd.DataFrame(out_array).to_csv(csv_path, index=False, header=False)\n",
    "                print(f\"Saved CSV: {csv_filename} in {csv_folder}\")\n",
    "                \n",
    "            except (KeyError, IndexError) as e:\n",
    "                print(f\"Error processing table: {e}\")\n",
    "                \n",
    "        elif element.get(\"type\") != \"Table\":\n",
    "            # Process text elements\n",
    "            text_content = element.get(\"text\", \"\")\n",
    "            if text_content:\n",
    "                text_folder = os.path.join(text_output_folder, f\"{pdf_name}-Texts\")\n",
    "                os.makedirs(text_folder, exist_ok=True)\n",
    "                text_filename = f\"{page_name}_text.txt\"\n",
    "                text_path = os.path.join(text_folder, text_filename)\n",
    "                \n",
    "                with open(text_path, \"a\", encoding=\"utf-8\") as text_file:\n",
    "                    text_file.write(text_content + \"\\n\")\n",
    "\n",
    "    # 6.4 Initialize OCR\n",
    "    ocr = PaddleOCR(lang='en')\n",
    "    \n",
    "    # 6.5 Process PDFs\n",
    "    pdf_files = [f for f in os.listdir(input_dir) if f.lower().endswith('.pdf')]\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(input_dir, pdf_file)\n",
    "        pdf_name = os.path.splitext(pdf_file)[0]\n",
    "        print(f\"\\nProcessing PDF: {pdf_file}\")\n",
    "        \n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        \n",
    "        for page_number in range(pdf_document.page_count):\n",
    "            page = pdf_document[page_number]\n",
    "            page_name = f'page_{page_number + 1}'\n",
    "            \n",
    "            # Extract images\n",
    "            extract_embedded_images(pdf_document, page, pdf_name, page_number)\n",
    "            \n",
    "            # Convert page to image\n",
    "            pix = page.get_pixmap(dpi=dpi, colorspace=fitz.csRGB)\n",
    "            img_array = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, 3)\n",
    "            image_cv = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Process page\n",
    "            temp_image_path = os.path.join(output_base_dir, \"temp_page.png\")\n",
    "            cv2.imwrite(temp_image_path, image_cv)\n",
    "            \n",
    "            try:\n",
    "                elements = partition_image(filename=temp_image_path, \n",
    "                                        infer_table_structure=True, \n",
    "                                        strategy='hi_res')\n",
    "                element_dict = [el.to_dict() for el in elements]\n",
    "                \n",
    "                for element in element_dict:\n",
    "                    process_element(element, page_name, pdf_name, image_cv)\n",
    "                    \n",
    "            finally:\n",
    "                if os.path.exists(temp_image_path):\n",
    "                    os.remove(temp_image_path)\n",
    "            \n",
    "            print(f\"Processed page {page_number + 1}/{pdf_document.page_count}\")\n",
    "        \n",
    "        pdf_document.close()\n",
    "        print(f\"Finished processing PDF: {pdf_file}\")\n",
    "\n",
    "    print(\"\\nProcessing complete!\")\n",
    "    print(f\"Text extracted to: {text_output_folder}\")\n",
    "    print(f\"Tables extracted to: {tables_output_folder}\")\n",
    "    print(f\"Table CSVs saved to: {tables_csv_folder}\")\n",
    "    print(f\"Embedded images extracted to: {images_output_folder}\")\n",
    "\n",
    "# 7. EXECUTION\n",
    "#------------\n",
    "# 7.1 Process with Default Settings\n",
    "process_pdf_documents_update(input_dir, output_dir)\n",
    "\n",
    "# 7.2 Process with Custom Settings (commented out)\n",
    "# process_pdf_documents_update(input_dir, output_dir, \n",
    "#                            pad_left=pad_left,\n",
    "#                            pad_top=pad_top, \n",
    "#                            pad_right=pad_right,\n",
    "#                            pad_bottom=pad_bottom)\n",
    "\n",
    "# 8. RESULTS VERIFICATION (Optional)\n",
    "#---------------------------------\n",
    "# 8.1 Load and Display Sample Results\n",
    "# df1 = pd.read_csv('PDF1 Tables/budget_speech-Pages-Cropped/page_40_Table_1.csv')\n",
    "# df1\n",
    "\n",
    "# 8.2 Load Another Sample\n",
    "# df2 = pd.read_csv('PDF1 Tables/budget_speech-Pages-Cropped/page_31_Table_1.csv')\n",
    "# df2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
